{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/WongKinYiu/yolov9.git\n",
    "%cd yolov9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-14T19:07:56.716462Z",
     "iopub.status.busy": "2024-06-14T19:07:56.715161Z",
     "iopub.status.idle": "2024-06-14T19:08:04.510405Z",
     "shell.execute_reply": "2024-06-14T19:08:04.509402Z",
     "shell.execute_reply.started": "2024-06-14T19:07:56.716412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T17:19:31.375479Z",
     "iopub.status.busy": "2024-06-15T17:19:31.374223Z",
     "iopub.status.idle": "2024-06-15T17:19:35.940504Z",
     "shell.execute_reply": "2024-06-15T17:19:35.939553Z",
     "shell.execute_reply.started": "2024-06-15T17:19:31.375419Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget -P weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-e.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.7),\n",
    "    A.GaussianBlur(blur_limit=(3, 7), p=0.7),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.7),\n",
    "    A.RandomGamma(gamma_limit=(80, 120), p=0.7),\n",
    "    A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=0.7),\n",
    "    A.ToGray(p=0.7),\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.7),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "def read_yolo_labels(label_path):\n",
    "    with open(label_path, 'r') as f:\n",
    "        labels = [line.strip() for line in f.readlines()]\n",
    "    return labels\n",
    "\n",
    "def write_yolo_labels(label_path, labels):\n",
    "    with open(label_path, 'w') as f:\n",
    "        for label in labels:\n",
    "            f.write(f\"{label}\\n\")\n",
    "\n",
    "def augment_image_on_cuda(image):\n",
    "    image_gpu = torch.from_numpy(image).cuda().float().permute(2, 0, 1) / 255.0\n",
    "    image_cpu = image_gpu.permute(1, 2, 0).cpu().numpy() * 255.0\n",
    "    image_cpu = image_cpu.astype(np.uint8)\n",
    "    \n",
    "    return image_cpu\n",
    "\n",
    "def main(image_path, label_path, output_image_dir, output_label_dir, augmented_num):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    labels = read_yolo_labels(label_path)\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "\n",
    "    for i in range(augmented_num):\n",
    "        transformed_image = augment_image_on_cuda(image)\n",
    "        augmented = transform(image=transformed_image)\n",
    "        transformed_image = augmented['image'].permute(1, 2, 0).cpu().numpy() * 255.0\n",
    "\n",
    "        transformed_image = transformed_image.astype(np.uint8)\n",
    "        transformed_image = cv2.cvtColor(transformed_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        output_image_path = os.path.join(output_image_dir, f\"{base_name}_{i}.jpg\")\n",
    "        output_label_path = os.path.join(output_label_dir, f\"{base_name}_{i}.txt\")\n",
    "\n",
    "        cv2.imwrite(output_image_path, transformed_image)\n",
    "        write_yolo_labels(output_label_path, labels)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define directories\n",
    "    input_image_dir = 'velding_defects'\n",
    "    input_label_dir = 'velding_defects'\n",
    "    output_image_dir = 'augmented_velding_defects'\n",
    "    output_label_dir = 'augmented_velding_defects'\n",
    "    \n",
    "    if not os.path.exists(output_image_dir):\n",
    "        os.makedirs(output_image_dir)\n",
    "    if not os.path.exists(output_label_dir):\n",
    "        os.makedirs(output_label_dir)\n",
    "\n",
    "    for image_name in os.listdir(input_image_dir):\n",
    "        if image_name.endswith('.jpg'):\n",
    "            base_name = os.path.splitext(image_name)[0]\n",
    "            image_path = os.path.join(input_image_dir, image_name)\n",
    "            label_path = os.path.join(input_label_dir, base_name + '.txt')\n",
    "\n",
    "            if os.path.exists(label_path):\n",
    "                main(image_path, label_path, output_image_dir, output_label_dir, augmented_num=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T21:57:54.854758Z",
     "iopub.status.busy": "2024-06-15T21:57:54.853378Z",
     "iopub.status.idle": "2024-06-15T21:58:00.468879Z",
     "shell.execute_reply": "2024-06-15T21:58:00.467990Z",
     "shell.execute_reply.started": "2024-06-15T21:57:54.854708Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files have been moved successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "base_dir = 'velding_defects'  # Путь к датасету\n",
    "images_dir = os.path.join(base_dir, 'images')\n",
    "labels_dir = os.path.join(base_dir, 'labels')\n",
    "\n",
    "# Необходимые папки\n",
    "image_train_dir = os.path.join(images_dir, 'train')\n",
    "image_val_dir = os.path.join(images_dir, 'val')\n",
    "label_train_dir = os.path.join(labels_dir, 'train')\n",
    "label_val_dir = os.path.join(labels_dir, 'val')\n",
    "\n",
    "# Создание папок\n",
    "os.makedirs(image_train_dir, exist_ok=True)\n",
    "os.makedirs(image_val_dir, exist_ok=True)\n",
    "os.makedirs(label_train_dir, exist_ok=True)\n",
    "os.makedirs(label_val_dir, exist_ok=True)\n",
    "\n",
    "# Получение списка всех файлов изображений\n",
    "image_files = [f for f in os.listdir(base_dir) if f.endswith('.jpg')]\n",
    "\n",
    "# Перемешивание и разделение файлов\n",
    "random.shuffle(image_files)\n",
    "split_index = int(0.7 * len(image_files))\n",
    "\n",
    "train_files = image_files[:split_index]\n",
    "val_files = image_files[split_index:]\n",
    "\n",
    "# Функция перемещения файлов, если существует соответствующая метка\n",
    "def move_files(file_list, target_image_dir, target_label_dir):\n",
    "    for file in file_list:\n",
    "        image_path = os.path.join(base_dir, file)\n",
    "        label_name = os.path.splitext(file)[0] + '.txt'\n",
    "        label_path = os.path.join(base_dir, label_name)\n",
    "        \n",
    "        if os.path.exists(label_path):\n",
    "            shutil.move(image_path, target_image_dir)\n",
    "            shutil.move(label_path, target_label_dir)\n",
    "        else:\n",
    "            print(f\"Label for {file} not found, skipping.\")\n",
    "\n",
    "# Перемещение файлов в соответствующие каталоги\n",
    "move_files(train_files, image_train_dir, label_train_dir)\n",
    "move_files(val_files, image_val_dir, label_val_dir)\n",
    "\n",
    "print(\"Files have been moved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T21:58:49.193971Z",
     "iopub.status.busy": "2024-06-15T21:58:49.192802Z",
     "iopub.status.idle": "2024-06-15T21:58:49.255544Z",
     "shell.execute_reply": "2024-06-15T21:58:49.254717Z",
     "shell.execute_reply.started": "2024-06-15T21:58:49.193921Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файлы velding_defects/train.txt и velding_defects/val.txt успешно созданы.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Укажите базовую директорию\n",
    "base_dir = Path('velding_defects')\n",
    "train_images_dir = base_dir / 'images' / 'train'\n",
    "val_images_dir = base_dir / 'images' / 'val'\n",
    "\n",
    "# Пути к файлам train.txt и val.txt\n",
    "train_txt_path = base_dir / 'train.txt'\n",
    "val_txt_path = base_dir / 'val.txt'\n",
    "\n",
    "# Функция для записи списка файлов с полными путями\n",
    "def write_file_list(file_list, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for file in file_list:\n",
    "            f.write(str(file) + '\\n')\n",
    "\n",
    "# Функция для получения списка всех .jpg изображений в директории\n",
    "def get_jpg_files(directory):\n",
    "    return list(directory.rglob('*.jpg'))\n",
    "\n",
    "# Получение всех .jpg изображений в тренировочных и валидационных директориях\n",
    "train_images = get_jpg_files(train_images_dir)\n",
    "val_images = get_jpg_files(val_images_dir)\n",
    "\n",
    "# Запись списков файлов в train.txt и val.txt с полными путями\n",
    "write_file_list(train_images, train_txt_path)\n",
    "write_file_list(val_images, val_txt_path)\n",
    "\n",
    "print(f\"Файлы {train_txt_path} и {val_txt_path} успешно созданы.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd yolov9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T22:04:35.757128Z",
     "iopub.status.busy": "2024-06-15T22:04:35.756097Z",
     "iopub.status.idle": "2024-06-15T23:29:52.209535Z",
     "shell.execute_reply": "2024-06-15T23:29:52.208610Z",
     "shell.execute_reply.started": "2024-06-15T22:04:35.757093Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "2024-06-15 22:04:48.432122: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-15 22:04:48.432122: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-15 22:04:50.152300: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-15 22:04:50.152300: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-15 22:04:55.619843: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-06-15 22:04:55.619854: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34m\u001b[1mtrain_dual: \u001b[0mweights=/home/jupyter/work/resources/weights/yolov9-e.pt, cfg=models/detect/yolov9-e.yaml, data=velding_defects/data.yaml, hyp=hyp.scratch-high.yaml, epochs=30, batch_size=32, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=0,1, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, flat_cos_lr=False, fixed_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, min_items=0, close_mosaic=5, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "YOLO 🚀 2024-6-15 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla V100-SXM2-32GB, 32501MiB)\n",
      "                                                   CUDA:1 (Tesla V100-SXM2-32GB, 32501MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, cls_pw=1.0, obj=0.7, obj_pw=1.0, dfl=1.5, iou_t=0.2, anchor_t=5.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.3\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLO 🚀 in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLO 🚀 runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=5\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1         0  models.common.Silence                   []                            \n",
      "  1                -1  1      1856  models.common.Conv                      [3, 64, 3, 2]                 \n",
      "  2                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  3                -1  1    252160  models.common.RepNCSPELAN4              [128, 256, 128, 64, 2]        \n",
      "  4                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
      "  5                -1  1   1004032  models.common.RepNCSPELAN4              [256, 512, 256, 128, 2]       \n",
      "  6                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      "  7                -1  1   4006912  models.common.RepNCSPELAN4              [512, 1024, 512, 256, 2]      \n",
      "  8                -1  1   2623488  models.common.ADown                     [1024, 1024]                  \n",
      "  9                -1  1   4269056  models.common.RepNCSPELAN4              [1024, 1024, 512, 256, 2]     \n",
      " 10                 1  1      4160  models.common.CBLinear                  [64, [64]]                    \n",
      " 11                 3  1     49344  models.common.CBLinear                  [256, [64, 128]]              \n",
      " 12                 5  1    229824  models.common.CBLinear                  [512, [64, 128, 256]]         \n",
      " 13                 7  1    984000  models.common.CBLinear                  [1024, [64, 128, 256, 512]]   \n",
      " 14                 9  1   2033600  models.common.CBLinear                  [1024, [64, 128, 256, 512, 1024]]\n",
      " 15                 0  1      1856  models.common.Conv                      [3, 64, 3, 2]                 \n",
      " 16[10, 11, 12, 13, 14, -1]  1         0  models.common.CBFuse                    [[0, 0, 0, 0, 0]]             \n",
      " 17                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      " 18[11, 12, 13, 14, -1]  1         0  models.common.CBFuse                    [[1, 1, 1, 1]]                \n",
      " 19                -1  1    252160  models.common.RepNCSPELAN4              [128, 256, 128, 64, 2]        \n",
      " 20                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
      " 21  [12, 13, 14, -1]  1         0  models.common.CBFuse                    [[2, 2, 2]]                   \n",
      " 22                -1  1   1004032  models.common.RepNCSPELAN4              [256, 512, 256, 128, 2]       \n",
      " 23                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      " 24      [13, 14, -1]  1         0  models.common.CBFuse                    [[3, 3]]                      \n",
      " 25                -1  1   4006912  models.common.RepNCSPELAN4              [512, 1024, 512, 256, 2]      \n",
      " 26                -1  1   2623488  models.common.ADown                     [1024, 1024]                  \n",
      " 27          [14, -1]  1         0  models.common.CBFuse                    [[4]]                         \n",
      " 28                -1  1   4269056  models.common.RepNCSPELAN4              [1024, 1024, 512, 256, 2]     \n",
      " 29                 9  1    787968  models.common.SPPELAN                   [1024, 512, 256]              \n",
      " 30                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 31           [-1, 7]  1         0  models.common.Concat                    [1]                           \n",
      " 32                -1  1   4005888  models.common.RepNCSPELAN4              [1536, 512, 512, 256, 2]      \n",
      " 33                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 34           [-1, 5]  1         0  models.common.Concat                    [1]                           \n",
      " 35                -1  1   1069056  models.common.RepNCSPELAN4              [1024, 256, 256, 128, 2]      \n",
      " 36                28  1    787968  models.common.SPPELAN                   [1024, 512, 256]              \n",
      " 37                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 38          [-1, 25]  1         0  models.common.Concat                    [1]                           \n",
      " 39                -1  1   4005888  models.common.RepNCSPELAN4              [1536, 512, 512, 256, 2]      \n",
      " 40                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 41          [-1, 22]  1         0  models.common.Concat                    [1]                           \n",
      " 42                -1  1   1069056  models.common.RepNCSPELAN4              [1024, 256, 256, 128, 2]      \n",
      " 43                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
      " 44          [-1, 39]  1         0  models.common.Concat                    [1]                           \n",
      " 45                -1  1   3612672  models.common.RepNCSPELAN4              [768, 512, 512, 256, 2]       \n",
      " 46                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      " 47          [-1, 36]  1         0  models.common.Concat                    [1]                           \n",
      " 48                -1  1  12860416  models.common.RepNCSPELAN4              [1024, 512, 1024, 512, 2]     \n",
      " 49[35, 32, 29, 42, 45, 48]  1  10988990  models.yolo.DualDDetect                 [5, [256, 512, 512, 256, 512, 512]]\n",
      "yolov9-e summary: 1475 layers, 69414014 parameters, 69413982 gradients, 244.9 GFLOPs\n",
      "\n",
      "Transferred 2160/2172 items from /home/jupyter/work/resources/weights/yolov9-e.pt\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 356 weight(decay=0.0), 375 weight(decay=0.0005), 373 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/jupyter/work/resources/yolov9/velding_defects/train... 2440 images, 283 backgrounds, 0 corrupt: 100%|██████████| 2440/2440 02:23\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/jupyter/work/resources/yolov9/velding_defects/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/jupyter/work/resources/yolov9/velding_defects/val... 1046 images, 137 backgrounds, 0 corrupt: 100%|██████████| 1046/1046 01:16\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/jupyter/work/resources/yolov9/velding_defects/val.cache\n",
      "Plotting labels to runs/train/exp12/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 16 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp12\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       0/29      22.7G      3.562      9.762      2.833        108        640:   0%|          | 0/77 00:26WARNING ⚠️ TensorBoard graph visualization failure Only tensors, lists, tuples of tensors, or dictionary of tensors can be output from traced functions\n",
      "       0/29      24.7G      3.306      5.648      2.445         25        640: 100%|██████████| 77/77 02:41\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:17\n",
      "                   all       1046       3051      0.345      0.231      0.186     0.0709\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/29        31G      2.732      2.823      1.947         16        640: 100%|██████████| 77/77 02:28\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:13\n",
      "                   all       1046       3051      0.484      0.398      0.392      0.159\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/29        31G      2.649      2.379      1.899         18        640: 100%|██████████| 77/77 02:27\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:15\n",
      "                   all       1046       3051      0.428      0.318       0.29      0.111\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/29        31G      2.669      2.338      1.871         41        640: 100%|██████████| 77/77 02:25\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:15\n",
      "                   all       1046       3051      0.393       0.27        0.2     0.0678\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/29      31.9G      2.622       2.17      1.845         10        640: 100%|██████████| 77/77 02:29\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:14\n",
      "                   all       1046       3051      0.442      0.332      0.323      0.129\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/29      31.9G      2.603      2.071      1.846         19        640: 100%|██████████| 77/77 02:32\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:15\n",
      "                   all       1046       3051      0.661      0.476      0.531      0.222\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/29      31.9G      2.518      1.925      1.815         35        640: 100%|██████████| 77/77 02:28\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:15\n",
      "                   all       1046       3051      0.675      0.542      0.582      0.256\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/29      31.9G      2.462      1.801      1.762         44        640: 100%|██████████| 77/77 02:29\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:15\n",
      "                   all       1046       3051      0.558      0.438      0.441      0.174\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/29      31.9G      2.414       1.67      1.728         24        640: 100%|██████████| 77/77 02:27\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:16\n",
      "                   all       1046       3051      0.745       0.56       0.64      0.277\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/29      31.9G      2.316      1.588      1.703         22        640: 100%|██████████| 77/77 02:31\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:15\n",
      "                   all       1046       3051      0.761      0.654      0.709      0.318\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/29      31.9G      2.309      1.533      1.699         16        640: 100%|██████████| 77/77 02:29\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:16\n",
      "                   all       1046       3051      0.712      0.628       0.67      0.307\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/29      31.9G       2.25      1.473      1.667         33        640: 100%|██████████| 77/77 02:31\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:14\n",
      "                   all       1046       3051      0.766       0.59      0.662      0.312\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/29      31.9G      2.207      1.414       1.64         29        640: 100%|██████████| 77/77 02:29\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:15\n",
      "                   all       1046       3051      0.845      0.727      0.795       0.38\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/29      31.9G      2.165      1.378      1.627         22        640: 100%|██████████| 77/77 02:28\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:16\n",
      "                   all       1046       3051      0.829      0.679      0.753      0.357\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/29      31.9G      2.119      1.306      1.606         44        640: 100%|██████████| 77/77 02:26\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:14\n",
      "                   all       1046       3051      0.828      0.775      0.822      0.416\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/29      31.9G      2.089      1.269       1.59         15        640: 100%|██████████| 77/77 02:26\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:14\n",
      "                   all       1046       3051      0.862      0.747      0.827      0.416\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/29      31.9G      2.063      1.238      1.573         83        640: 100%|██████████| 77/77 02:29\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:15\n",
      "                   all       1046       3051      0.848      0.796      0.847      0.437\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/29      31.9G      2.056      1.203      1.565         31        640: 100%|██████████| 77/77 02:27\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:15\n",
      "                   all       1046       3051      0.848      0.797      0.859       0.44\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/29      31.9G      2.014      1.164      1.553         21        640: 100%|██████████| 77/77 02:29\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:15\n",
      "                   all       1046       3051      0.857      0.808      0.864      0.452\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/29      31.9G      1.987      1.144      1.535         19        640: 100%|██████████| 77/77 02:30\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:14\n",
      "                   all       1046       3051      0.871       0.81      0.876      0.467\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      20/29      31.9G      1.944      1.115      1.499         29        640: 100%|██████████| 77/77 02:29\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:15\n",
      "                   all       1046       3051      0.892      0.812      0.885      0.476\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      21/29      31.9G      1.907      1.072      1.482         29        640: 100%|██████████| 77/77 02:28\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:14\n",
      "                   all       1046       3051      0.907      0.819      0.892      0.477\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      22/29      31.9G      1.892      1.076       1.48         20        640: 100%|██████████| 77/77 02:28\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:15\n",
      "                   all       1046       3051      0.892      0.829      0.894      0.485\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      23/29      31.9G      1.868      1.028      1.464         22        640: 100%|██████████| 77/77 02:28\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:14\n",
      "                   all       1046       3051      0.894      0.834      0.904      0.483\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      24/29      31.9G      1.821       1.01      1.445         11        640: 100%|██████████| 77/77 02:30\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:16\n",
      "                   all       1046       3051      0.879       0.86      0.899      0.499\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      25/29      31.9G      1.731      0.872      1.508          7        640: 100%|██████████| 77/77 01:04\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:15\n",
      "                   all       1046       3051      0.871      0.782      0.847       0.45\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      26/29      31.9G      1.694     0.8325      1.505         18        640: 100%|██████████| 77/77 01:05\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:14\n",
      "                   all       1046       3051      0.897      0.862      0.913      0.498\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      27/29      31.9G      1.661     0.8069      1.487          8        640: 100%|██████████| 77/77 01:04\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:15\n",
      "                   all       1046       3051      0.897      0.851      0.914      0.517\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      28/29      31.9G       1.62      0.772      1.442         11        640: 100%|██████████| 77/77 01:04\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:16\n",
      "                   all       1046       3051      0.904      0.866      0.926      0.525\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      29/29      31.9G      1.582      0.742      1.446         10        640: 100%|██████████| 77/77 01:04\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:16\n",
      "                   all       1046       3051      0.915      0.883       0.93      0.537\n",
      "\n",
      "30 epochs completed in 1.331 hours.\n",
      "Optimizer stripped from runs/train/exp12/weights/last.pt, 139.9MB\n",
      "Optimizer stripped from runs/train/exp12/weights/best.pt, 139.9MB\n",
      "\n",
      "Validating runs/train/exp12/weights/best.pt...\n",
      "Fusing layers... \n",
      "yolov9-e summary: 839 layers, 68553982 parameters, 0 gradients, 240.7 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:15\n",
      "                   all       1046       3051      0.914      0.884       0.93      0.537\n",
      "                   adj       1046       1338      0.878      0.916      0.915      0.445\n",
      "                   int       1046        368      0.909      0.707      0.814      0.441\n",
      "                   geo       1046        937      0.933      0.942      0.983      0.668\n",
      "                   pro       1046        248      0.909      0.923      0.961      0.502\n",
      "                   non       1046        160      0.941      0.931      0.975      0.631\n",
      "Results saved to \u001b[1mruns/train/exp12\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m torch.distributed.run --nproc_per_node 2 train_dual.py \\\n",
    "--batch 32 --epochs 30 --img 640 --device 0,1 --min-items 0 --close-mosaic 5 \\\n",
    "--data velding_defects/data.yaml \\\n",
    "--weights /home/jupyter/work/resources/weights/yolov9-e.pt \\\n",
    "--cfg models/detect/yolov9-e.yaml \\\n",
    "--hyp hyp.scratch-high.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T00:20:07.961386Z",
     "iopub.status.busy": "2024-06-16T00:20:07.960430Z",
     "iopub.status.idle": "2024-06-16T01:20:43.258243Z",
     "shell.execute_reply": "2024-06-16T01:20:43.257212Z",
     "shell.execute_reply.started": "2024-06-16T00:20:07.961352Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "2024-06-16 00:20:21.217490: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-16 00:20:21.217493: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-16 00:20:22.926929: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-16 00:20:22.926935: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-16 00:20:28.031960: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-06-16 00:20:28.031962: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34m\u001b[1mtrain_dual: \u001b[0mweights=/home/jupyter/work/resources/yolov9/runs/train/exp12/weights/best.pt, cfg=models/detect/yolov9-e.yaml, data=velding_defects/data.yaml, hyp=hyp.scratch-high.yaml, epochs=22, batch_size=32, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=0,1, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, flat_cos_lr=False, fixed_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, min_items=0, close_mosaic=5, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "YOLO 🚀 2024-6-15 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla V100-SXM2-32GB, 32501MiB)\n",
      "                                                   CUDA:1 (Tesla V100-SXM2-32GB, 32501MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, cls_pw=1.0, obj=0.7, obj_pw=1.0, dfl=1.5, iou_t=0.2, anchor_t=5.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.3\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLO 🚀 in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLO 🚀 runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=5\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1         0  models.common.Silence                   []                            \n",
      "  1                -1  1      1856  models.common.Conv                      [3, 64, 3, 2]                 \n",
      "  2                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  3                -1  1    252160  models.common.RepNCSPELAN4              [128, 256, 128, 64, 2]        \n",
      "  4                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
      "  5                -1  1   1004032  models.common.RepNCSPELAN4              [256, 512, 256, 128, 2]       \n",
      "  6                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      "  7                -1  1   4006912  models.common.RepNCSPELAN4              [512, 1024, 512, 256, 2]      \n",
      "  8                -1  1   2623488  models.common.ADown                     [1024, 1024]                  \n",
      "  9                -1  1   4269056  models.common.RepNCSPELAN4              [1024, 1024, 512, 256, 2]     \n",
      " 10                 1  1      4160  models.common.CBLinear                  [64, [64]]                    \n",
      " 11                 3  1     49344  models.common.CBLinear                  [256, [64, 128]]              \n",
      " 12                 5  1    229824  models.common.CBLinear                  [512, [64, 128, 256]]         \n",
      " 13                 7  1    984000  models.common.CBLinear                  [1024, [64, 128, 256, 512]]   \n",
      " 14                 9  1   2033600  models.common.CBLinear                  [1024, [64, 128, 256, 512, 1024]]\n",
      " 15                 0  1      1856  models.common.Conv                      [3, 64, 3, 2]                 \n",
      " 16[10, 11, 12, 13, 14, -1]  1         0  models.common.CBFuse                    [[0, 0, 0, 0, 0]]             \n",
      " 17                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      " 18[11, 12, 13, 14, -1]  1         0  models.common.CBFuse                    [[1, 1, 1, 1]]                \n",
      " 19                -1  1    252160  models.common.RepNCSPELAN4              [128, 256, 128, 64, 2]        \n",
      " 20                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
      " 21  [12, 13, 14, -1]  1         0  models.common.CBFuse                    [[2, 2, 2]]                   \n",
      " 22                -1  1   1004032  models.common.RepNCSPELAN4              [256, 512, 256, 128, 2]       \n",
      " 23                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      " 24      [13, 14, -1]  1         0  models.common.CBFuse                    [[3, 3]]                      \n",
      " 25                -1  1   4006912  models.common.RepNCSPELAN4              [512, 1024, 512, 256, 2]      \n",
      " 26                -1  1   2623488  models.common.ADown                     [1024, 1024]                  \n",
      " 27          [14, -1]  1         0  models.common.CBFuse                    [[4]]                         \n",
      " 28                -1  1   4269056  models.common.RepNCSPELAN4              [1024, 1024, 512, 256, 2]     \n",
      " 29                 9  1    787968  models.common.SPPELAN                   [1024, 512, 256]              \n",
      " 30                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 31           [-1, 7]  1         0  models.common.Concat                    [1]                           \n",
      " 32                -1  1   4005888  models.common.RepNCSPELAN4              [1536, 512, 512, 256, 2]      \n",
      " 33                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 34           [-1, 5]  1         0  models.common.Concat                    [1]                           \n",
      " 35                -1  1   1069056  models.common.RepNCSPELAN4              [1024, 256, 256, 128, 2]      \n",
      " 36                28  1    787968  models.common.SPPELAN                   [1024, 512, 256]              \n",
      " 37                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 38          [-1, 25]  1         0  models.common.Concat                    [1]                           \n",
      " 39                -1  1   4005888  models.common.RepNCSPELAN4              [1536, 512, 512, 256, 2]      \n",
      " 40                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 41          [-1, 22]  1         0  models.common.Concat                    [1]                           \n",
      " 42                -1  1   1069056  models.common.RepNCSPELAN4              [1024, 256, 256, 128, 2]      \n",
      " 43                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
      " 44          [-1, 39]  1         0  models.common.Concat                    [1]                           \n",
      " 45                -1  1   3612672  models.common.RepNCSPELAN4              [768, 512, 512, 256, 2]       \n",
      " 46                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      " 47          [-1, 36]  1         0  models.common.Concat                    [1]                           \n",
      " 48                -1  1  12860416  models.common.RepNCSPELAN4              [1024, 512, 1024, 512, 2]     \n",
      " 49[35, 32, 29, 42, 45, 48]  1  10988990  models.yolo.DualDDetect                 [5, [256, 512, 512, 256, 512, 512]]\n",
      "yolov9-e summary: 1475 layers, 69414014 parameters, 69413982 gradients, 244.9 GFLOPs\n",
      "\n",
      "Transferred 2172/2172 items from /home/jupyter/work/resources/yolov9/runs/train/exp12/weights/best.pt\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 356 weight(decay=0.0), 375 weight(decay=0.0005), 373 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/jupyter/work/resources/yolov9/velding_defects/train.cache... 2440 images, 283 backgrounds, 0 corrupt: 100%|██████████| 2440/2440 00:00\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/jupyter/work/resources/yolov9/velding_defects/val.cache... 1046 images, 137 backgrounds, 0 corrupt: 100%|██████████| 1046/1046 00:00\n",
      "Plotting labels to runs/train/exp13/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 16 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp13\u001b[0m\n",
      "Starting training for 22 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       0/21      22.7G      1.684     0.8023      1.378        108        640:   0%|          | 0/77 01:54WARNING ⚠️ TensorBoard graph visualization failure Only tensors, lists, tuples of tensors, or dictionary of tensors can be output from traced functions\n",
      "       0/21      24.7G       1.72     0.9032      1.396         25        640: 100%|██████████| 77/77 04:45\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:40\n",
      "                   all       1046       3051      0.919      0.854      0.917      0.528\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/21        31G      1.748     0.9426       1.41         16        640: 100%|██████████| 77/77 02:28\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:15\n",
      "                   all       1046       3051      0.891      0.873      0.916      0.513\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/21        31G      1.832       1.02      1.463         18        640: 100%|██████████| 77/77 02:29\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:16\n",
      "                   all       1046       3051       0.89      0.828      0.886       0.47\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/21        31G      1.998      1.198      1.518         41        640: 100%|██████████| 77/77 02:25\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:16\n",
      "                   all       1046       3051      0.838      0.716      0.785      0.388\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/21        31G      1.991      1.225      1.522         10        640: 100%|██████████| 77/77 02:28\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:15\n",
      "                   all       1046       3051      0.807       0.74      0.799      0.396\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/21        31G      2.047      1.256      1.555         19        640: 100%|██████████| 77/77 02:30\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:15\n",
      "                   all       1046       3051      0.814      0.721      0.772      0.374\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/21        31G      1.998      1.186      1.543         35        640: 100%|██████████| 77/77 02:31\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:16\n",
      "                   all       1046       3051      0.853      0.788      0.856       0.44\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/21        31G      1.979      1.185      1.519         44        640: 100%|██████████| 77/77 02:29\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:15\n",
      "                   all       1046       3051      0.828      0.782       0.83      0.428\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/21        31G      1.965       1.13      1.504         24        640: 100%|██████████| 77/77 02:26\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:16\n",
      "                   all       1046       3051      0.872      0.802      0.868      0.448\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/21        31G      1.901      1.092       1.49         22        640: 100%|██████████| 77/77 02:31\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:14\n",
      "                   all       1046       3051      0.872      0.809      0.878       0.46\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/21        31G      1.915      1.083      1.493         16        640: 100%|██████████| 77/77 02:29\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:14\n",
      "                   all       1046       3051      0.868      0.801      0.859      0.447\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/21        31G      1.857      1.032      1.464         33        640: 100%|██████████| 77/77 02:30\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:16\n",
      "                   all       1046       3051      0.898      0.833      0.888       0.48\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/21        31G      1.831      1.018      1.453         29        640: 100%|██████████| 77/77 02:29\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:14\n",
      "                   all       1046       3051      0.889      0.863       0.91      0.495\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/21        31G      1.778     0.9787      1.438         22        640: 100%|██████████| 77/77 02:28\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:16\n",
      "                   all       1046       3051      0.897      0.824      0.895      0.481\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/21        31G      1.736     0.9306      1.411         44        640: 100%|██████████| 77/77 02:27\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:15\n",
      "                   all       1046       3051      0.904      0.883      0.919      0.518\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/21        31G      1.707       0.91      1.403         15        640: 100%|██████████| 77/77 02:25\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:16\n",
      "                   all       1046       3051      0.894      0.822      0.898      0.486\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/21        31G      1.701      0.909      1.402         83        640: 100%|██████████| 77/77 02:30\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:16\n",
      "                   all       1046       3051      0.906      0.883      0.927      0.534\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/21      31.9G       1.64     0.7938      1.469         16        640: 100%|██████████| 77/77 01:02\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:14\n",
      "                   all       1046       3051      0.894      0.854       0.91      0.512\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/21      31.9G      1.596     0.7661      1.433          4        640: 100%|██████████| 77/77 01:03\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:15\n",
      "                   all       1046       3051       0.91      0.883      0.933      0.531\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/21      31.9G      1.551     0.7328      1.415         16        640: 100%|██████████| 77/77 01:02\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:14\n",
      "                   all       1046       3051       0.92      0.882      0.937      0.547\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      20/21      31.9G      1.523     0.7117      1.394         20        640: 100%|██████████| 77/77 01:02\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:16\n",
      "                   all       1046       3051      0.939      0.897      0.945      0.563\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      21/21      31.9G      1.516     0.6867      1.388          7        640: 100%|██████████| 77/77 01:02\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:16\n",
      "                   all       1046       3051      0.928      0.909      0.949      0.568\n",
      "\n",
      "22 epochs completed in 0.980 hours.\n",
      "Optimizer stripped from runs/train/exp13/weights/last.pt, 139.9MB\n",
      "Optimizer stripped from runs/train/exp13/weights/best.pt, 139.9MB\n",
      "\n",
      "Validating runs/train/exp13/weights/best.pt...\n",
      "Fusing layers... \n",
      "yolov9-e summary: 839 layers, 68553982 parameters, 0 gradients, 240.7 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:16\n",
      "                   all       1046       3051      0.928      0.909      0.949      0.568\n",
      "                   adj       1046       1338      0.918      0.911      0.933      0.467\n",
      "                   int       1046        368      0.894      0.802      0.874      0.488\n",
      "                   geo       1046        937       0.94      0.963      0.986      0.685\n",
      "                   pro       1046        248      0.925      0.919      0.961       0.54\n",
      "                   non       1046        160      0.963       0.95      0.991      0.662\n",
      "Results saved to \u001b[1mruns/train/exp13\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m torch.distributed.run --nproc_per_node 2 train_dual.py \\\n",
    "--batch 32 --epochs 22 --img 640 --device 0,1 --min-items 0 --close-mosaic 5 \\\n",
    "--data velding_defects/data.yaml \\\n",
    "--weights /home/jupyter/work/resources/yolov9/runs/train/exp12/weights/best.pt \\\n",
    "--cfg models/detect/yolov9-e.yaml \\\n",
    "--hyp hyp.scratch-high.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T01:28:15.131534Z",
     "iopub.status.busy": "2024-06-16T01:28:15.130485Z",
     "iopub.status.idle": "2024-06-16T02:37:55.742558Z",
     "shell.execute_reply": "2024-06-16T02:37:55.741407Z",
     "shell.execute_reply.started": "2024-06-16T01:28:15.131498Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-16 01:28:20.936423: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-16 01:28:20.936423: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-16 01:28:21.003268: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-16 01:28:21.003292: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-16 01:28:22.098631: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-06-16 01:28:22.098635: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34m\u001b[1mtrain_dual: \u001b[0mweights=/home/jupyter/work/resources/yolov9/runs/train/exp13/weights/best.pt, cfg=models/detect/yolov9-e.yaml, data=velding_defects/data.yaml, hyp=hyp.scratch-high.yaml, epochs=25, batch_size=32, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=0,1, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, flat_cos_lr=False, fixed_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, min_items=0, close_mosaic=0, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "YOLO 🚀 2024-6-15 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla V100-SXM2-32GB, 32501MiB)\n",
      "                                                   CUDA:1 (Tesla V100-SXM2-32GB, 32501MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, cls_pw=1.0, obj=0.7, obj_pw=1.0, dfl=1.5, iou_t=0.2, anchor_t=5.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.3\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLO 🚀 in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLO 🚀 runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=5\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1         0  models.common.Silence                   []                            \n",
      "  1                -1  1      1856  models.common.Conv                      [3, 64, 3, 2]                 \n",
      "  2                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  3                -1  1    252160  models.common.RepNCSPELAN4              [128, 256, 128, 64, 2]        \n",
      "  4                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
      "  5                -1  1   1004032  models.common.RepNCSPELAN4              [256, 512, 256, 128, 2]       \n",
      "  6                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      "  7                -1  1   4006912  models.common.RepNCSPELAN4              [512, 1024, 512, 256, 2]      \n",
      "  8                -1  1   2623488  models.common.ADown                     [1024, 1024]                  \n",
      "  9                -1  1   4269056  models.common.RepNCSPELAN4              [1024, 1024, 512, 256, 2]     \n",
      " 10                 1  1      4160  models.common.CBLinear                  [64, [64]]                    \n",
      " 11                 3  1     49344  models.common.CBLinear                  [256, [64, 128]]              \n",
      " 12                 5  1    229824  models.common.CBLinear                  [512, [64, 128, 256]]         \n",
      " 13                 7  1    984000  models.common.CBLinear                  [1024, [64, 128, 256, 512]]   \n",
      " 14                 9  1   2033600  models.common.CBLinear                  [1024, [64, 128, 256, 512, 1024]]\n",
      " 15                 0  1      1856  models.common.Conv                      [3, 64, 3, 2]                 \n",
      " 16[10, 11, 12, 13, 14, -1]  1         0  models.common.CBFuse                    [[0, 0, 0, 0, 0]]             \n",
      " 17                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      " 18[11, 12, 13, 14, -1]  1         0  models.common.CBFuse                    [[1, 1, 1, 1]]                \n",
      " 19                -1  1    252160  models.common.RepNCSPELAN4              [128, 256, 128, 64, 2]        \n",
      " 20                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
      " 21  [12, 13, 14, -1]  1         0  models.common.CBFuse                    [[2, 2, 2]]                   \n",
      " 22                -1  1   1004032  models.common.RepNCSPELAN4              [256, 512, 256, 128, 2]       \n",
      " 23                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      " 24      [13, 14, -1]  1         0  models.common.CBFuse                    [[3, 3]]                      \n",
      " 25                -1  1   4006912  models.common.RepNCSPELAN4              [512, 1024, 512, 256, 2]      \n",
      " 26                -1  1   2623488  models.common.ADown                     [1024, 1024]                  \n",
      " 27          [14, -1]  1         0  models.common.CBFuse                    [[4]]                         \n",
      " 28                -1  1   4269056  models.common.RepNCSPELAN4              [1024, 1024, 512, 256, 2]     \n",
      " 29                 9  1    787968  models.common.SPPELAN                   [1024, 512, 256]              \n",
      " 30                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 31           [-1, 7]  1         0  models.common.Concat                    [1]                           \n",
      " 32                -1  1   4005888  models.common.RepNCSPELAN4              [1536, 512, 512, 256, 2]      \n",
      " 33                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 34           [-1, 5]  1         0  models.common.Concat                    [1]                           \n",
      " 35                -1  1   1069056  models.common.RepNCSPELAN4              [1024, 256, 256, 128, 2]      \n",
      " 36                28  1    787968  models.common.SPPELAN                   [1024, 512, 256]              \n",
      " 37                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 38          [-1, 25]  1         0  models.common.Concat                    [1]                           \n",
      " 39                -1  1   4005888  models.common.RepNCSPELAN4              [1536, 512, 512, 256, 2]      \n",
      " 40                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 41          [-1, 22]  1         0  models.common.Concat                    [1]                           \n",
      " 42                -1  1   1069056  models.common.RepNCSPELAN4              [1024, 256, 256, 128, 2]      \n",
      " 43                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
      " 44          [-1, 39]  1         0  models.common.Concat                    [1]                           \n",
      " 45                -1  1   3612672  models.common.RepNCSPELAN4              [768, 512, 512, 256, 2]       \n",
      " 46                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      " 47          [-1, 36]  1         0  models.common.Concat                    [1]                           \n",
      " 48                -1  1  12860416  models.common.RepNCSPELAN4              [1024, 512, 1024, 512, 2]     \n",
      " 49[35, 32, 29, 42, 45, 48]  1  10988990  models.yolo.DualDDetect                 [5, [256, 512, 512, 256, 512, 512]]\n",
      "yolov9-e summary: 1475 layers, 69414014 parameters, 69413982 gradients, 244.9 GFLOPs\n",
      "\n",
      "Transferred 2172/2172 items from /home/jupyter/work/resources/yolov9/runs/train/exp13/weights/best.pt\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 356 weight(decay=0.0), 375 weight(decay=0.0005), 373 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/jupyter/work/resources/yolov9/velding_defects/train.cache... 2440 images, 283 backgrounds, 0 corrupt: 100%|██████████| 2440/2440 00:00\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/jupyter/work/resources/yolov9/velding_defects/val.cache... 1046 images, 137 backgrounds, 0 corrupt: 100%|██████████| 1046/1046 00:00\n",
      "Plotting labels to runs/train/exp14/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 16 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp14\u001b[0m\n",
      "Starting training for 25 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       0/24      22.7G      1.597     0.7446      1.349        108        640:   0%|          | 0/77 00:16WARNING ⚠️ TensorBoard graph visualization failure Only tensors, lists, tuples of tensors, or dictionary of tensors can be output from traced functions\n",
      "       0/24      24.7G      1.612     0.8329      1.355         25        640: 100%|██████████| 77/77 02:39\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:22\n",
      "                   all       1046       3051      0.908       0.88      0.933      0.549\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/24        31G      1.644     0.8608      1.367         13        640: 100%|██████████| 77/77 02:06\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:25\n",
      "                   all       1046       3051      0.917      0.888      0.937      0.545\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/24        31G      1.718     0.9167       1.41         31        640: 100%|██████████| 77/77 02:08\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:27\n",
      "                   all       1046       3051      0.891      0.776      0.861      0.453\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/24        31G      1.846      1.041      1.454         35        640: 100%|██████████| 77/77 02:07\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:25\n",
      "                   all       1046       3051      0.837      0.765      0.825      0.411\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/24        31G        1.9      1.084      1.499         23        640: 100%|██████████| 77/77 02:05\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:26\n",
      "                   all       1046       3051      0.852       0.77      0.844      0.428\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/24        31G        1.9      1.106      1.493         31        640: 100%|██████████| 77/77 02:12\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:26\n",
      "                   all       1046       3051      0.877      0.794      0.847      0.444\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/24        31G      1.897      1.074      1.483         40        640: 100%|██████████| 77/77 02:05\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:29\n",
      "                   all       1046       3051      0.902      0.822      0.896      0.476\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/24        31G      1.867      1.057      1.461         21        640: 100%|██████████| 77/77 02:05\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:26\n",
      "                   all       1046       3051      0.827      0.706      0.767      0.388\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/24        31G      1.833      1.045      1.443         24        640: 100%|██████████| 77/77 02:09\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:27\n",
      "                   all       1046       3051       0.86      0.806      0.869      0.465\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/24      31.9G      1.813      1.004       1.44         21        640: 100%|██████████| 77/77 02:04\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:25\n",
      "                   all       1046       3051      0.885      0.844      0.902      0.487\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/24      31.9G       1.77      0.975      1.417         18        640: 100%|██████████| 77/77 02:07\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:25\n",
      "                   all       1046       3051      0.907      0.866      0.923      0.513\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/24      31.9G      1.771     0.9722      1.427          7        640: 100%|██████████| 77/77 02:10\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:24\n",
      "                   all       1046       3051      0.904      0.857      0.921       0.51\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/24      31.9G       1.75     0.9577      1.413         46        640: 100%|██████████| 77/77 02:10\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:24\n",
      "                   all       1046       3051      0.893      0.879      0.923      0.517\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/24      31.9G      1.708     0.9215       1.39         19        640: 100%|██████████| 77/77 02:07\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:25\n",
      "                   all       1046       3051      0.896      0.876       0.92      0.518\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/24      31.9G      1.695     0.9021      1.379         27        640: 100%|██████████| 77/77 02:07\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:24\n",
      "                   all       1046       3051      0.871      0.835      0.887      0.491\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/24      31.9G       1.67     0.8887      1.388         27        640: 100%|██████████| 77/77 02:12\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:25\n",
      "                   all       1046       3051      0.929      0.886      0.932      0.541\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/24      31.9G      1.649     0.8704      1.375         18        640: 100%|██████████| 77/77 02:07\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:26\n",
      "                   all       1046       3051      0.913       0.89      0.931      0.541\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/24      31.9G      1.612     0.8439      1.348         31        640: 100%|██████████| 77/77 02:08\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:25\n",
      "                   all       1046       3051       0.93      0.908       0.95      0.566\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/24      31.9G      1.585     0.8421      1.348         18        640: 100%|██████████| 77/77 02:09\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:25\n",
      "                   all       1046       3051      0.925      0.911      0.954      0.568\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/24      31.9G      1.565     0.8102      1.341         35        640: 100%|██████████| 77/77 02:09\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:24\n",
      "                   all       1046       3051      0.909      0.906      0.947      0.564\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      20/24      31.9G      1.552      0.789      1.336         26        640: 100%|██████████| 77/77 02:10\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:24\n",
      "                   all       1046       3051      0.932      0.911      0.958      0.581\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      21/24      31.9G       1.48     0.7612      1.317          9        640: 100%|██████████| 77/77 02:10\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:24\n",
      "                   all       1046       3051      0.917      0.919      0.953      0.576\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      22/24      31.9G      1.484     0.7561      1.295         24        640: 100%|██████████| 77/77 02:12\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:22\n",
      "                   all       1046       3051      0.932      0.917      0.958      0.589\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      23/24      31.9G      1.467     0.7494       1.31         17        640: 100%|██████████| 77/77 02:07\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:26\n",
      "                   all       1046       3051      0.944      0.932      0.964      0.599\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      24/24      31.9G      1.459     0.7399      1.291         25        640: 100%|██████████| 77/77 02:11\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:23\n",
      "                   all       1046       3051      0.948       0.93      0.965      0.608\n",
      "\n",
      "25 epochs completed in 1.137 hours.\n",
      "Optimizer stripped from runs/train/exp14/weights/last.pt, 139.9MB\n",
      "Optimizer stripped from runs/train/exp14/weights/best.pt, 139.9MB\n",
      "\n",
      "Validating runs/train/exp14/weights/best.pt...\n",
      "Fusing layers... \n",
      "yolov9-e summary: 839 layers, 68553982 parameters, 0 gradients, 240.7 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:15\n",
      "                   all       1046       3051      0.948       0.93      0.965      0.607\n",
      "                   adj       1046       1338      0.923      0.927      0.939      0.491\n",
      "                   int       1046        368       0.95      0.826      0.921      0.548\n",
      "                   geo       1046        937      0.961      0.975       0.99      0.737\n",
      "                   pro       1046        248      0.942      0.979      0.984      0.579\n",
      "                   non       1046        160      0.962      0.944       0.99      0.681\n",
      "Results saved to \u001b[1mruns/train/exp14\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m torch.distributed.run --nproc_per_node 2 train_dual.py \\\n",
    "--batch 32 --epochs 25 --img 640 --device 0,1 --min-items 0 --close-mosaic 0 \\\n",
    "--data velding_defects/data.yaml \\\n",
    "--weights /home/jupyter/work/resources/yolov9/runs/train/exp13/weights/best.pt \\\n",
    "--cfg models/detect/yolov9-e.yaml \\\n",
    "--hyp hyp.scratch-high.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T03:11:35.627311Z",
     "iopub.status.busy": "2024-06-16T03:11:35.626262Z",
     "iopub.status.idle": "2024-06-16T04:07:40.656339Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "2024-06-16 03:11:41.474366: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-16 03:11:41.474366: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-16 03:11:41.535008: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-16 03:11:41.535010: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-16 03:11:42.646071: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-06-16 03:11:42.647239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34m\u001b[1mtrain_dual: \u001b[0mweights=/home/jupyter/work/resources/yolov9/runs/train/exp14/weights/best.pt, cfg=models/detect/yolov9-e.yaml, data=velding_defects/data.yaml, hyp=hyp.scratch-high.yaml, epochs=20, batch_size=32, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=0,1, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, flat_cos_lr=False, fixed_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, min_items=0, close_mosaic=0, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "YOLO 🚀 2024-6-15 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla V100-SXM2-32GB, 32501MiB)\n",
      "                                                   CUDA:1 (Tesla V100-SXM2-32GB, 32501MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, cls_pw=1.0, obj=0.7, obj_pw=1.0, dfl=1.5, iou_t=0.2, anchor_t=5.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.3\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLO 🚀 in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLO 🚀 runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=5\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1         0  models.common.Silence                   []                            \n",
      "  1                -1  1      1856  models.common.Conv                      [3, 64, 3, 2]                 \n",
      "  2                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  3                -1  1    252160  models.common.RepNCSPELAN4              [128, 256, 128, 64, 2]        \n",
      "  4                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
      "  5                -1  1   1004032  models.common.RepNCSPELAN4              [256, 512, 256, 128, 2]       \n",
      "  6                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      "  7                -1  1   4006912  models.common.RepNCSPELAN4              [512, 1024, 512, 256, 2]      \n",
      "  8                -1  1   2623488  models.common.ADown                     [1024, 1024]                  \n",
      "  9                -1  1   4269056  models.common.RepNCSPELAN4              [1024, 1024, 512, 256, 2]     \n",
      " 10                 1  1      4160  models.common.CBLinear                  [64, [64]]                    \n",
      " 11                 3  1     49344  models.common.CBLinear                  [256, [64, 128]]              \n",
      " 12                 5  1    229824  models.common.CBLinear                  [512, [64, 128, 256]]         \n",
      " 13                 7  1    984000  models.common.CBLinear                  [1024, [64, 128, 256, 512]]   \n",
      " 14                 9  1   2033600  models.common.CBLinear                  [1024, [64, 128, 256, 512, 1024]]\n",
      " 15                 0  1      1856  models.common.Conv                      [3, 64, 3, 2]                 \n",
      " 16[10, 11, 12, 13, 14, -1]  1         0  models.common.CBFuse                    [[0, 0, 0, 0, 0]]             \n",
      " 17                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      " 18[11, 12, 13, 14, -1]  1         0  models.common.CBFuse                    [[1, 1, 1, 1]]                \n",
      " 19                -1  1    252160  models.common.RepNCSPELAN4              [128, 256, 128, 64, 2]        \n",
      " 20                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
      " 21  [12, 13, 14, -1]  1         0  models.common.CBFuse                    [[2, 2, 2]]                   \n",
      " 22                -1  1   1004032  models.common.RepNCSPELAN4              [256, 512, 256, 128, 2]       \n",
      " 23                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      " 24      [13, 14, -1]  1         0  models.common.CBFuse                    [[3, 3]]                      \n",
      " 25                -1  1   4006912  models.common.RepNCSPELAN4              [512, 1024, 512, 256, 2]      \n",
      " 26                -1  1   2623488  models.common.ADown                     [1024, 1024]                  \n",
      " 27          [14, -1]  1         0  models.common.CBFuse                    [[4]]                         \n",
      " 28                -1  1   4269056  models.common.RepNCSPELAN4              [1024, 1024, 512, 256, 2]     \n",
      " 29                 9  1    787968  models.common.SPPELAN                   [1024, 512, 256]              \n",
      " 30                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 31           [-1, 7]  1         0  models.common.Concat                    [1]                           \n",
      " 32                -1  1   4005888  models.common.RepNCSPELAN4              [1536, 512, 512, 256, 2]      \n",
      " 33                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 34           [-1, 5]  1         0  models.common.Concat                    [1]                           \n",
      " 35                -1  1   1069056  models.common.RepNCSPELAN4              [1024, 256, 256, 128, 2]      \n",
      " 36                28  1    787968  models.common.SPPELAN                   [1024, 512, 256]              \n",
      " 37                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 38          [-1, 25]  1         0  models.common.Concat                    [1]                           \n",
      " 39                -1  1   4005888  models.common.RepNCSPELAN4              [1536, 512, 512, 256, 2]      \n",
      " 40                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 41          [-1, 22]  1         0  models.common.Concat                    [1]                           \n",
      " 42                -1  1   1069056  models.common.RepNCSPELAN4              [1024, 256, 256, 128, 2]      \n",
      " 43                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
      " 44          [-1, 39]  1         0  models.common.Concat                    [1]                           \n",
      " 45                -1  1   3612672  models.common.RepNCSPELAN4              [768, 512, 512, 256, 2]       \n",
      " 46                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      " 47          [-1, 36]  1         0  models.common.Concat                    [1]                           \n",
      " 48                -1  1  12860416  models.common.RepNCSPELAN4              [1024, 512, 1024, 512, 2]     \n",
      " 49[35, 32, 29, 42, 45, 48]  1  10988990  models.yolo.DualDDetect                 [5, [256, 512, 512, 256, 512, 512]]\n",
      "yolov9-e summary: 1475 layers, 69414014 parameters, 69413982 gradients, 244.9 GFLOPs\n",
      "\n",
      "Transferred 2172/2172 items from /home/jupyter/work/resources/yolov9/runs/train/exp14/weights/best.pt\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 356 weight(decay=0.0), 375 weight(decay=0.0005), 373 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/jupyter/work/resources/yolov9/velding_defects/train.cache... 2440 images, 283 backgrounds, 0 corrupt: 100%|██████████| 2440/2440 00:00\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/jupyter/work/resources/yolov9/velding_defects/val.cache... 1046 images, 137 backgrounds, 0 corrupt: 100%|██████████| 1046/1046 00:00\n",
      "Plotting labels to runs/train/exp16/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 16 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp16\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       0/19      22.7G      1.338     0.6114      1.296        108        640:   0%|          | 0/77 00:15WARNING ⚠️ TensorBoard graph visualization failure Only tensors, lists, tuples of tensors, or dictionary of tensors can be output from traced functions\n",
      "       0/19      24.7G      1.428      0.711      1.285         25        640: 100%|██████████| 77/77 02:38\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:22\n",
      "                   all       1046       3051      0.923       0.88      0.942      0.571\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/19        31G       1.45     0.7374      1.287         13        640: 100%|██████████| 77/77 02:06\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:26\n",
      "                   all       1046       3051      0.941      0.918      0.954      0.582\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/19        31G       1.49     0.7739      1.312         31        640: 100%|██████████| 77/77 02:08\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:26\n",
      "                   all       1046       3051      0.869      0.827      0.891      0.502\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/19        31G      1.624     0.8596       1.35         35        640: 100%|██████████| 77/77 02:06\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:25\n",
      "                   all       1046       3051      0.864      0.782      0.847      0.448\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/19        31G      1.683     0.9016      1.399         23        640: 100%|██████████| 77/77 02:05\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:26\n",
      "                   all       1046       3051      0.867      0.794      0.858      0.465\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/19        31G      1.694     0.9328       1.39         31        640: 100%|██████████| 77/77 02:10\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:27\n",
      "                   all       1046       3051      0.896      0.866       0.92      0.507\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/19        31G      1.679     0.9037      1.379         40        640: 100%|██████████| 77/77 02:07\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:27\n",
      "                   all       1046       3051      0.914      0.874      0.925      0.528\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/19        31G      1.664     0.8947      1.368         21        640: 100%|██████████| 77/77 02:07\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:24\n",
      "                   all       1046       3051      0.832      0.749      0.811      0.415\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/19        31G      1.647     0.8819      1.357         24        640: 100%|██████████| 77/77 02:10\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:26\n",
      "                   all       1046       3051      0.914      0.865       0.93      0.531\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/19      31.9G      1.611     0.8574      1.348         21        640: 100%|██████████| 77/77 02:06\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:25\n",
      "                   all       1046       3051      0.902      0.863      0.916      0.514\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/19      31.9G      1.569     0.8223      1.331         18        640: 100%|██████████| 77/77 02:08\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:26\n",
      "                   all       1046       3051      0.916       0.88       0.93      0.544\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/19      31.9G      1.562     0.8178      1.332          7        640: 100%|██████████| 77/77 02:09\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:26\n",
      "                   all       1046       3051      0.902      0.879      0.927      0.544\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/19      31.9G       1.55     0.8034      1.322         46        640: 100%|██████████| 77/77 02:07\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:25\n",
      "                   all       1046       3051      0.842       0.78      0.841      0.471\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/19      31.9G      1.493     0.7746      1.293         19        640: 100%|██████████| 77/77 02:07\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:24\n",
      "                   all       1046       3051      0.922      0.924      0.952       0.58\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/19      31.9G      1.485     0.7543      1.289         27        640: 100%|██████████| 77/77 02:08\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:25\n",
      "                   all       1046       3051      0.932      0.913      0.951      0.579\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/19      31.9G      1.456     0.7428      1.294         27        640: 100%|██████████| 77/77 02:13\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:24\n",
      "                   all       1046       3051      0.935      0.915      0.954       0.59\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/19      31.9G      1.435     0.7368      1.284         18        640: 100%|██████████| 77/77 02:07\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:26\n",
      "                   all       1046       3051      0.919      0.928      0.954      0.593\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/19      31.9G      1.398     0.7155      1.265         31        640: 100%|██████████| 77/77 02:07\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:25\n",
      "                   all       1046       3051      0.926      0.931      0.956      0.603\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/19      31.9G      1.373     0.6976       1.26         18        640: 100%|██████████| 77/77 02:11\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:25\n",
      "                   all       1046       3051      0.942      0.931      0.963      0.609\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/19      31.9G      1.357     0.6796      1.254         35        640: 100%|██████████| 77/77 02:10\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:24\n",
      "                   all       1046       3051      0.937      0.936      0.967      0.618\n",
      "\n",
      "20 epochs completed in 0.913 hours.\n",
      "Optimizer stripped from runs/train/exp16/weights/last.pt, 139.9MB\n",
      "Optimizer stripped from runs/train/exp16/weights/best.pt, 139.9MB\n",
      "\n",
      "Validating runs/train/exp16/weights/best.pt...\n",
      "Fusing layers... \n",
      "yolov9-e summary: 839 layers, 68553982 parameters, 0 gradients, 240.7 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:15\n",
      "                   all       1046       3051      0.937      0.936      0.967      0.618\n",
      "                   adj       1046       1338      0.919      0.928      0.934      0.504\n",
      "                   int       1046        368      0.911      0.859      0.938      0.566\n",
      "                   geo       1046        937      0.965       0.98      0.992      0.751\n",
      "                   pro       1046        248      0.934      0.972      0.981      0.591\n",
      "                   non       1046        160      0.956      0.942       0.99      0.677\n",
      "Results saved to \u001b[1mruns/train/exp16\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m torch.distributed.run --nproc_per_node 2 train_dual.py \\\n",
    "--batch 32 --epochs 20 --img 640 --device 0,1 --min-items 0 --close-mosaic 0 \\\n",
    "--data velding_defects/data.yaml \\\n",
    "--weights /home/jupyter/work/resources/yolov9/runs/train/exp14/weights/best.pt \\\n",
    "--cfg models/detect/yolov9-e.yaml \\\n",
    "--hyp hyp.scratch-high.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T04:16:19.773198Z",
     "iopub.status.busy": "2024-06-16T04:16:19.771949Z",
     "iopub.status.idle": "2024-06-16T05:12:24.713390Z",
     "shell.execute_reply": "2024-06-16T05:12:24.712501Z",
     "shell.execute_reply.started": "2024-06-16T04:16:19.773161Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-16 04:16:25.597498: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-16 04:16:25.597498: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-16 04:16:25.664225: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-16 04:16:25.664225: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-16 04:16:26.787198: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-06-16 04:16:26.788423: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34m\u001b[1mtrain_dual: \u001b[0mweights=/home/jupyter/work/resources/yolov9/runs/train/exp16/weights/best.pt, cfg=models/detect/yolov9-e.yaml, data=velding_defects/data.yaml, hyp=hyp.scratch-high.yaml, epochs=20, batch_size=32, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=0,1, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, flat_cos_lr=False, fixed_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, min_items=0, close_mosaic=0, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "YOLO 🚀 2024-6-15 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla V100-SXM2-32GB, 32501MiB)\n",
      "                                                   CUDA:1 (Tesla V100-SXM2-32GB, 32501MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, cls_pw=1.0, obj=0.7, obj_pw=1.0, dfl=1.5, iou_t=0.2, anchor_t=5.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.3\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLO 🚀 in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLO 🚀 runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=5\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1         0  models.common.Silence                   []                            \n",
      "  1                -1  1      1856  models.common.Conv                      [3, 64, 3, 2]                 \n",
      "  2                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  3                -1  1    252160  models.common.RepNCSPELAN4              [128, 256, 128, 64, 2]        \n",
      "  4                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
      "  5                -1  1   1004032  models.common.RepNCSPELAN4              [256, 512, 256, 128, 2]       \n",
      "  6                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      "  7                -1  1   4006912  models.common.RepNCSPELAN4              [512, 1024, 512, 256, 2]      \n",
      "  8                -1  1   2623488  models.common.ADown                     [1024, 1024]                  \n",
      "  9                -1  1   4269056  models.common.RepNCSPELAN4              [1024, 1024, 512, 256, 2]     \n",
      " 10                 1  1      4160  models.common.CBLinear                  [64, [64]]                    \n",
      " 11                 3  1     49344  models.common.CBLinear                  [256, [64, 128]]              \n",
      " 12                 5  1    229824  models.common.CBLinear                  [512, [64, 128, 256]]         \n",
      " 13                 7  1    984000  models.common.CBLinear                  [1024, [64, 128, 256, 512]]   \n",
      " 14                 9  1   2033600  models.common.CBLinear                  [1024, [64, 128, 256, 512, 1024]]\n",
      " 15                 0  1      1856  models.common.Conv                      [3, 64, 3, 2]                 \n",
      " 16[10, 11, 12, 13, 14, -1]  1         0  models.common.CBFuse                    [[0, 0, 0, 0, 0]]             \n",
      " 17                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      " 18[11, 12, 13, 14, -1]  1         0  models.common.CBFuse                    [[1, 1, 1, 1]]                \n",
      " 19                -1  1    252160  models.common.RepNCSPELAN4              [128, 256, 128, 64, 2]        \n",
      " 20                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
      " 21  [12, 13, 14, -1]  1         0  models.common.CBFuse                    [[2, 2, 2]]                   \n",
      " 22                -1  1   1004032  models.common.RepNCSPELAN4              [256, 512, 256, 128, 2]       \n",
      " 23                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      " 24      [13, 14, -1]  1         0  models.common.CBFuse                    [[3, 3]]                      \n",
      " 25                -1  1   4006912  models.common.RepNCSPELAN4              [512, 1024, 512, 256, 2]      \n",
      " 26                -1  1   2623488  models.common.ADown                     [1024, 1024]                  \n",
      " 27          [14, -1]  1         0  models.common.CBFuse                    [[4]]                         \n",
      " 28                -1  1   4269056  models.common.RepNCSPELAN4              [1024, 1024, 512, 256, 2]     \n",
      " 29                 9  1    787968  models.common.SPPELAN                   [1024, 512, 256]              \n",
      " 30                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 31           [-1, 7]  1         0  models.common.Concat                    [1]                           \n",
      " 32                -1  1   4005888  models.common.RepNCSPELAN4              [1536, 512, 512, 256, 2]      \n",
      " 33                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 34           [-1, 5]  1         0  models.common.Concat                    [1]                           \n",
      " 35                -1  1   1069056  models.common.RepNCSPELAN4              [1024, 256, 256, 128, 2]      \n",
      " 36                28  1    787968  models.common.SPPELAN                   [1024, 512, 256]              \n",
      " 37                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 38          [-1, 25]  1         0  models.common.Concat                    [1]                           \n",
      " 39                -1  1   4005888  models.common.RepNCSPELAN4              [1536, 512, 512, 256, 2]      \n",
      " 40                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 41          [-1, 22]  1         0  models.common.Concat                    [1]                           \n",
      " 42                -1  1   1069056  models.common.RepNCSPELAN4              [1024, 256, 256, 128, 2]      \n",
      " 43                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
      " 44          [-1, 39]  1         0  models.common.Concat                    [1]                           \n",
      " 45                -1  1   3612672  models.common.RepNCSPELAN4              [768, 512, 512, 256, 2]       \n",
      " 46                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
      " 47          [-1, 36]  1         0  models.common.Concat                    [1]                           \n",
      " 48                -1  1  12860416  models.common.RepNCSPELAN4              [1024, 512, 1024, 512, 2]     \n",
      " 49[35, 32, 29, 42, 45, 48]  1  10988990  models.yolo.DualDDetect                 [5, [256, 512, 512, 256, 512, 512]]\n",
      "yolov9-e summary: 1475 layers, 69414014 parameters, 69413982 gradients, 244.9 GFLOPs\n",
      "\n",
      "Transferred 2172/2172 items from /home/jupyter/work/resources/yolov9/runs/train/exp16/weights/best.pt\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 356 weight(decay=0.0), 375 weight(decay=0.0005), 373 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/jupyter/work/resources/yolov9/velding_defects/train.cache... 2440 images, 283 backgrounds, 0 corrupt: 100%|██████████| 2440/2440 00:00\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/jupyter/work/resources/yolov9/velding_defects/val.cache... 1046 images, 137 backgrounds, 0 corrupt: 100%|██████████| 1046/1046 00:00\n",
      "Plotting labels to runs/train/exp17/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 16 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp17\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       0/19      22.7G      1.284     0.6046      1.239        108        640:   0%|          | 0/77 00:15WARNING ⚠️ TensorBoard graph visualization failure Only tensors, lists, tuples of tensors, or dictionary of tensors can be output from traced functions\n",
      "       0/19      24.7G      1.355     0.6736      1.251         25        640: 100%|██████████| 77/77 02:42\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:21\n",
      "                   all       1046       3051      0.943      0.905      0.956      0.592\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/19        31G      1.341     0.6772       1.24         13        640: 100%|██████████| 77/77 02:07\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:25\n",
      "                   all       1046       3051      0.938      0.905      0.953      0.593\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/19        31G      1.348     0.6877      1.253         31        640: 100%|██████████| 77/77 02:06\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:27\n",
      "                   all       1046       3051      0.898      0.828      0.901      0.506\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/19        31G      1.484     0.7676      1.291         35        640: 100%|██████████| 77/77 02:07\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:24\n",
      "                   all       1046       3051      0.888      0.843      0.904      0.499\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/19        31G      1.552     0.8143       1.34         23        640: 100%|██████████| 77/77 02:05\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:26\n",
      "                   all       1046       3051      0.886      0.862        0.9      0.502\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/19        31G      1.572     0.8483      1.338         31        640: 100%|██████████| 77/77 02:10\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:27\n",
      "                   all       1046       3051      0.886      0.864      0.918      0.521\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/19        31G      1.571     0.8227      1.331         40        640: 100%|██████████| 77/77 02:08\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:26\n",
      "                   all       1046       3051      0.913      0.885      0.935      0.549\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/19        31G      1.549     0.8108      1.314         21        640: 100%|██████████| 77/77 02:06\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:26\n",
      "                   all       1046       3051       0.88      0.868      0.906      0.505\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/19        31G      1.532     0.8136      1.311         24        640: 100%|██████████| 77/77 02:10\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:26\n",
      "                   all       1046       3051      0.881      0.824      0.889      0.493\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/19      31.9G      1.499     0.7869      1.302         21        640: 100%|██████████| 77/77 02:05\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:25\n",
      "                   all       1046       3051      0.897      0.855      0.905      0.506\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/19      31.9G      1.467      0.758      1.292         18        640: 100%|██████████| 77/77 02:08\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:25\n",
      "                   all       1046       3051      0.909      0.889      0.928      0.556\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/19      31.9G      1.462     0.7541      1.297          7        640: 100%|██████████| 77/77 02:09\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:25\n",
      "                   all       1046       3051      0.927      0.886      0.938      0.563\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/19      31.9G      1.438     0.7454      1.281         46        640: 100%|██████████| 77/77 02:09\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:25\n",
      "                   all       1046       3051      0.929        0.9      0.945      0.574\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/19      31.9G      1.396     0.7115      1.261         19        640: 100%|██████████| 77/77 02:07\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:25\n",
      "                   all       1046       3051      0.925      0.926      0.957      0.592\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/19      31.9G      1.392     0.7035      1.259         27        640: 100%|██████████| 77/77 02:06\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:25\n",
      "                   all       1046       3051      0.935      0.889      0.949      0.576\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/19      31.9G      1.369     0.6969      1.264         27        640: 100%|██████████| 77/77 02:14\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:24\n",
      "                   all       1046       3051      0.936      0.921      0.957      0.604\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/19      31.9G      1.348     0.6894      1.255         18        640: 100%|██████████| 77/77 02:05\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:25\n",
      "                   all       1046       3051       0.94      0.928      0.963      0.614\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/19      31.9G       1.32      0.674      1.234         31        640: 100%|██████████| 77/77 02:06\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:24\n",
      "                   all       1046       3051      0.936      0.935      0.962      0.624\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/19      31.9G      1.298     0.6577      1.231         18        640: 100%|██████████| 77/77 02:10\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:25\n",
      "                   all       1046       3051      0.929      0.933      0.959      0.615\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/19      31.9G      1.299     0.6489      1.234         35        640: 100%|██████████| 77/77 02:10\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:23\n",
      "                   all       1046       3051      0.952      0.938      0.969      0.634\n",
      "\n",
      "20 epochs completed in 0.912 hours.\n",
      "Optimizer stripped from runs/train/exp17/weights/last.pt, 139.9MB\n",
      "Optimizer stripped from runs/train/exp17/weights/best.pt, 139.9MB\n",
      "\n",
      "Validating runs/train/exp17/weights/best.pt...\n",
      "Fusing layers... \n",
      "yolov9-e summary: 839 layers, 68553982 parameters, 0 gradients, 240.7 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:17\n",
      "                   all       1046       3051      0.952      0.938      0.969      0.634\n",
      "                   adj       1046       1338      0.923      0.931      0.939      0.516\n",
      "                   int       1046        368      0.931      0.864      0.939      0.587\n",
      "                   geo       1046        937      0.972      0.979      0.991       0.76\n",
      "                   pro       1046        248      0.953      0.974      0.987      0.617\n",
      "                   non       1046        160      0.979      0.944      0.991       0.69\n",
      "Results saved to \u001b[1mruns/train/exp17\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m torch.distributed.run --nproc_per_node 2 train_dual.py \\\n",
    "--batch 32 --epochs 20 --img 640 --device 0,1 --min-items 0 --close-mosaic 0 \\\n",
    "--data velding_defects/data.yaml \\\n",
    "--weights /home/jupyter/work/resources/yolov9/runs/train/exp16/weights/best.pt \\\n",
    "--cfg models/detect/yolov9-e.yaml \\\n",
    "--hyp hyp.scratch-high.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T05:47:23.327313Z",
     "iopub.status.busy": "2024-06-16T05:47:23.326254Z",
     "iopub.status.idle": "2024-06-16T05:47:23.360812Z",
     "shell.execute_reply": "2024-06-16T05:47:23.360058Z",
     "shell.execute_reply.started": "2024-06-16T05:47:23.327277Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/work/resources\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T05:47:27.096147Z",
     "iopub.status.busy": "2024-06-16T05:47:27.094738Z",
     "iopub.status.idle": "2024-06-16T05:47:42.895268Z",
     "shell.execute_reply": "2024-06-16T05:47:42.894467Z",
     "shell.execute_reply.started": "2024-06-16T05:47:27.096104Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLO 🚀 2024-6-15 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla V100-SXM2-32GB, 32501MiB)\n",
      "\n",
      "Fusing layers... \n",
      "yolov9-c summary: 604 layers, 50707518 parameters, 0 gradients, 236.7 GFLOPs\n",
      "image 1/363 /home/jupyter/work/resources/test_dataset/0.jpg: 384x640 1 geo, 40.9ms\n",
      "image 2/363 /home/jupyter/work/resources/test_dataset/1 (1).jpg: 384x640 2 ints, 3 geos, 21.4ms\n",
      "image 3/363 /home/jupyter/work/resources/test_dataset/1 (101).jpg: 384x640 2 geos, 21.7ms\n",
      "image 4/363 /home/jupyter/work/resources/test_dataset/1 (102).jpg: 384x640 4 geos, 21.6ms\n",
      "image 5/363 /home/jupyter/work/resources/test_dataset/1 (103).jpg: 384x640 1 int, 5 geos, 21.3ms\n",
      "image 6/363 /home/jupyter/work/resources/test_dataset/1 (104).jpg: 384x640 1 int, 4 geos, 21.2ms\n",
      "image 7/363 /home/jupyter/work/resources/test_dataset/1 (107).jpg: 384x640 1 int, 1 geo, 20.8ms\n",
      "image 8/363 /home/jupyter/work/resources/test_dataset/1 (108).jpg: 384x640 1 int, 1 geo, 20.6ms\n",
      "image 9/363 /home/jupyter/work/resources/test_dataset/1 (109).jpg: 384x640 2 ints, 20.8ms\n",
      "image 10/363 /home/jupyter/work/resources/test_dataset/1 (110).jpg: 384x640 2 ints, 1 geo, 20.9ms\n",
      "image 11/363 /home/jupyter/work/resources/test_dataset/1 (13).jpg: 384x640 1 int, 20.8ms\n",
      "image 12/363 /home/jupyter/work/resources/test_dataset/1 (14).jpg: 384x640 1 int, 1 geo, 20.5ms\n",
      "image 13/363 /home/jupyter/work/resources/test_dataset/1 (19).jpg: 384x640 1 int, 20.3ms\n",
      "image 14/363 /home/jupyter/work/resources/test_dataset/1 (2).jpg: 384x640 1 int, 1 geo, 20.3ms\n",
      "image 15/363 /home/jupyter/work/resources/test_dataset/1 (20).jpg: 384x640 1 int, 1 geo, 20.5ms\n",
      "image 16/363 /home/jupyter/work/resources/test_dataset/1 (25).jpg: 384x640 1 geo, 20.5ms\n",
      "image 17/363 /home/jupyter/work/resources/test_dataset/1 (26).jpg: 384x640 1 geo, 20.5ms\n",
      "image 18/363 /home/jupyter/work/resources/test_dataset/1 (27).jpg: 384x640 1 int, 1 pro, 20.4ms\n",
      "image 19/363 /home/jupyter/work/resources/test_dataset/1 (28).jpg: 384x640 1 int, 21.2ms\n",
      "image 20/363 /home/jupyter/work/resources/test_dataset/1 (31).jpg: 384x640 1 int, 2 pros, 20.7ms\n",
      "image 21/363 /home/jupyter/work/resources/test_dataset/1 (32).jpg: 384x640 2 ints, 1 pro, 20.5ms\n",
      "image 22/363 /home/jupyter/work/resources/test_dataset/1 (35).jpg: 384x640 2 ints, 2 pros, 20.7ms\n",
      "image 23/363 /home/jupyter/work/resources/test_dataset/1 (36).jpg: 384x640 4 ints, 1 pro, 20.6ms\n",
      "image 24/363 /home/jupyter/work/resources/test_dataset/1 (39).jpg: 384x640 1 pro, 20.4ms\n",
      "image 25/363 /home/jupyter/work/resources/test_dataset/1 (4).jpg: 384x640 1 int, 20.9ms\n",
      "image 26/363 /home/jupyter/work/resources/test_dataset/1 (40).jpg: 384x640 1 geo, 1 pro, 20.7ms\n",
      "image 27/363 /home/jupyter/work/resources/test_dataset/1 (43).jpg: 384x640 1 int, 1 geo, 2 pros, 20.3ms\n",
      "image 28/363 /home/jupyter/work/resources/test_dataset/1 (44).jpg: 384x640 2 ints, 1 geo, 20.4ms\n",
      "image 29/363 /home/jupyter/work/resources/test_dataset/1 (47).jpg: 384x640 2 pros, 20.6ms\n",
      "image 30/363 /home/jupyter/work/resources/test_dataset/1 (48).jpg: 384x640 1 geo, 1 pro, 20.7ms\n",
      "image 31/363 /home/jupyter/work/resources/test_dataset/1 (51).jpg: 384x640 1 int, 1 geo, 1 pro, 20.6ms\n",
      "image 32/363 /home/jupyter/work/resources/test_dataset/1 (52).jpg: 384x640 2 ints, 2 pros, 20.4ms\n",
      "image 33/363 /home/jupyter/work/resources/test_dataset/1 (55).jpg: 384x640 2 ints, 2 pros, 20.5ms\n",
      "image 34/363 /home/jupyter/work/resources/test_dataset/1 (56).jpg: 384x640 3 pros, 20.4ms\n",
      "image 35/363 /home/jupyter/work/resources/test_dataset/1 (59).jpg: 384x640 2 ints, 2 geos, 3 pros, 20.4ms\n",
      "image 36/363 /home/jupyter/work/resources/test_dataset/1 (60).jpg: 384x640 1 int, 1 geo, 4 pros, 20.7ms\n",
      "image 37/363 /home/jupyter/work/resources/test_dataset/1 (63).jpg: 384x640 2 pros, 20.8ms\n",
      "image 38/363 /home/jupyter/work/resources/test_dataset/1 (64).jpg: 384x640 1 int, 1 geo, 2 pros, 20.7ms\n",
      "image 39/363 /home/jupyter/work/resources/test_dataset/1 (67).jpg: 384x640 1 int, 3 geos, 1 pro, 20.7ms\n",
      "image 40/363 /home/jupyter/work/resources/test_dataset/1 (68).jpg: 384x640 1 int, 4 geos, 20.7ms\n",
      "image 41/363 /home/jupyter/work/resources/test_dataset/1 (7).jpg: 384x640 1 int, 1 pro, 21.0ms\n",
      "image 42/363 /home/jupyter/work/resources/test_dataset/1 (71).jpg: 384x640 1 int, 1 geo, 1 pro, 21.6ms\n",
      "image 43/363 /home/jupyter/work/resources/test_dataset/1 (72).jpg: 384x640 2 ints, 1 geo, 21.1ms\n",
      "image 44/363 /home/jupyter/work/resources/test_dataset/1 (75).jpg: 384x640 2 ints, 4 geos, 20.7ms\n",
      "image 45/363 /home/jupyter/work/resources/test_dataset/1 (76).jpg: 384x640 3 ints, 2 geos, 1 pro, 20.7ms\n",
      "image 46/363 /home/jupyter/work/resources/test_dataset/1 (79).jpg: 384x640 1 int, 2 geos, 1 pro, 20.7ms\n",
      "image 47/363 /home/jupyter/work/resources/test_dataset/1 (8).jpg: 384x640 1 int, 20.6ms\n",
      "image 48/363 /home/jupyter/work/resources/test_dataset/1 (80).jpg: 384x640 3 geos, 2 pros, 20.6ms\n",
      "image 49/363 /home/jupyter/work/resources/test_dataset/1 (83).jpg: 384x640 1 int, 3 geos, 21.0ms\n",
      "image 50/363 /home/jupyter/work/resources/test_dataset/1 (84).jpg: 384x640 4 geos, 21.0ms\n",
      "image 51/363 /home/jupyter/work/resources/test_dataset/1 (87).jpg: 384x640 1 int, 2 geos, 20.9ms\n",
      "image 52/363 /home/jupyter/work/resources/test_dataset/1 (88).jpg: 384x640 2 geos, 1 pro, 20.7ms\n",
      "image 53/363 /home/jupyter/work/resources/test_dataset/1 (89).jpg: 384x640 2 ints, 1 pro, 20.7ms\n",
      "image 54/363 /home/jupyter/work/resources/test_dataset/1 (90).jpg: 384x640 1 geo, 1 pro, 20.8ms\n",
      "image 55/363 /home/jupyter/work/resources/test_dataset/1 (91).jpg: 384x640 1 int, 2 geos, 1 pro, 20.6ms\n",
      "image 56/363 /home/jupyter/work/resources/test_dataset/1 (92).jpg: 384x640 1 int, 1 geo, 20.8ms\n",
      "image 57/363 /home/jupyter/work/resources/test_dataset/1 (93).jpg: 384x640 2 ints, 2 geos, 21.7ms\n",
      "image 58/363 /home/jupyter/work/resources/test_dataset/1 (94).jpg: 384x640 1 int, 2 geos, 21.2ms\n",
      "image 59/363 /home/jupyter/work/resources/test_dataset/1 (95).jpg: 384x640 1 int, 1 geo, 21.6ms\n",
      "image 60/363 /home/jupyter/work/resources/test_dataset/1 (96).jpg: 384x640 3 geos, 21.4ms\n",
      "image 61/363 /home/jupyter/work/resources/test_dataset/1 (97).jpg: 384x640 4 geos, 21.5ms\n",
      "image 62/363 /home/jupyter/work/resources/test_dataset/1 (98).jpg: 384x640 4 geos, 21.8ms\n",
      "image 63/363 /home/jupyter/work/resources/test_dataset/10.jpg: 384x640 3 ints, 1 pro, 21.4ms\n",
      "image 64/363 /home/jupyter/work/resources/test_dataset/100.jpg: 384x640 1 int, 21.6ms\n",
      "image 65/363 /home/jupyter/work/resources/test_dataset/101.jpg: 384x640 (no detections), 21.6ms\n",
      "image 66/363 /home/jupyter/work/resources/test_dataset/102.jpg: 384x640 (no detections), 21.8ms\n",
      "image 67/363 /home/jupyter/work/resources/test_dataset/103.jpg: 384x640 (no detections), 22.4ms\n",
      "image 68/363 /home/jupyter/work/resources/test_dataset/104.jpg: 384x640 (no detections), 21.6ms\n",
      "image 69/363 /home/jupyter/work/resources/test_dataset/105.jpg: 384x640 1 geo, 21.5ms\n",
      "image 70/363 /home/jupyter/work/resources/test_dataset/106.jpg: 384x640 (no detections), 21.9ms\n",
      "image 71/363 /home/jupyter/work/resources/test_dataset/107.jpg: 384x640 (no detections), 21.6ms\n",
      "image 72/363 /home/jupyter/work/resources/test_dataset/108.jpg: 384x640 1 pro, 21.7ms\n",
      "image 73/363 /home/jupyter/work/resources/test_dataset/109.jpg: 384x640 1 pro, 21.7ms\n",
      "image 74/363 /home/jupyter/work/resources/test_dataset/11.jpg: 384x640 1 int, 21.7ms\n",
      "image 75/363 /home/jupyter/work/resources/test_dataset/110.jpg: 384x640 1 int, 21.5ms\n",
      "image 76/363 /home/jupyter/work/resources/test_dataset/12.jpg: 384x640 (no detections), 21.7ms\n",
      "image 77/363 /home/jupyter/work/resources/test_dataset/13.jpg: 384x640 1 int, 21.7ms\n",
      "image 78/363 /home/jupyter/work/resources/test_dataset/14.jpg: 384x640 1 geo, 21.6ms\n",
      "image 79/363 /home/jupyter/work/resources/test_dataset/15.jpg: 384x640 (no detections), 21.8ms\n",
      "image 80/363 /home/jupyter/work/resources/test_dataset/16.jpg: 384x640 (no detections), 21.6ms\n",
      "image 81/363 /home/jupyter/work/resources/test_dataset/17.jpg: 384x640 1 geo, 21.5ms\n",
      "image 82/363 /home/jupyter/work/resources/test_dataset/18.jpg: 384x640 2 geos, 22.0ms\n",
      "image 83/363 /home/jupyter/work/resources/test_dataset/19.jpg: 384x640 (no detections), 21.2ms\n",
      "image 84/363 /home/jupyter/work/resources/test_dataset/2 (1).jpg: 384x640 4 pros, 20.8ms\n",
      "image 85/363 /home/jupyter/work/resources/test_dataset/2 (10).jpg: 384x640 1 int, 1 pro, 20.9ms\n",
      "image 86/363 /home/jupyter/work/resources/test_dataset/2 (11).jpg: 384x640 1 geo, 20.9ms\n",
      "image 87/363 /home/jupyter/work/resources/test_dataset/2 (12).jpg: 384x640 1 adj, 1 geo, 21.5ms\n",
      "image 88/363 /home/jupyter/work/resources/test_dataset/2 (13).jpg: 384x640 1 geo, 20.8ms\n",
      "image 89/363 /home/jupyter/work/resources/test_dataset/2 (14).jpg: 384x640 1 int, 1 pro, 20.6ms\n",
      "image 90/363 /home/jupyter/work/resources/test_dataset/2 (15).jpg: 384x640 3 geos, 1 pro, 20.6ms\n",
      "image 91/363 /home/jupyter/work/resources/test_dataset/2 (16).jpg: 384x640 1 geo, 20.7ms\n",
      "image 92/363 /home/jupyter/work/resources/test_dataset/2 (17).jpg: 384x640 1 int, 2 geos, 20.5ms\n",
      "image 93/363 /home/jupyter/work/resources/test_dataset/2 (18).jpg: 384x640 1 int, 2 geos, 21.0ms\n",
      "image 94/363 /home/jupyter/work/resources/test_dataset/2 (19).jpg: 384x640 1 geo, 20.5ms\n",
      "image 95/363 /home/jupyter/work/resources/test_dataset/2 (2).jpg: 384x640 1 geo, 20.6ms\n",
      "image 96/363 /home/jupyter/work/resources/test_dataset/2 (20).jpg: 384x640 2 geos, 20.6ms\n",
      "image 97/363 /home/jupyter/work/resources/test_dataset/2 (21).jpg: 384x640 1 geo, 20.7ms\n",
      "image 98/363 /home/jupyter/work/resources/test_dataset/2 (22).jpg: 384x640 1 geo, 20.8ms\n",
      "image 99/363 /home/jupyter/work/resources/test_dataset/2 (23).jpg: 384x640 1 geo, 21.3ms\n",
      "image 100/363 /home/jupyter/work/resources/test_dataset/2 (24).jpg: 384x640 1 int, 1 geo, 20.7ms\n",
      "image 101/363 /home/jupyter/work/resources/test_dataset/2 (25).jpg: 384x640 2 geos, 20.6ms\n",
      "image 102/363 /home/jupyter/work/resources/test_dataset/2 (26).jpg: 384x640 (no detections), 20.7ms\n",
      "image 103/363 /home/jupyter/work/resources/test_dataset/2 (27).jpg: 384x640 (no detections), 20.6ms\n",
      "image 104/363 /home/jupyter/work/resources/test_dataset/2 (28).jpg: 384x640 (no detections), 20.5ms\n",
      "image 105/363 /home/jupyter/work/resources/test_dataset/2 (29).jpg: 384x640 (no detections), 20.3ms\n",
      "image 106/363 /home/jupyter/work/resources/test_dataset/2 (3).jpg: 384x640 1 int, 2 geos, 20.7ms\n",
      "image 107/363 /home/jupyter/work/resources/test_dataset/2 (30).jpg: 384x640 (no detections), 20.5ms\n",
      "image 108/363 /home/jupyter/work/resources/test_dataset/2 (31).jpg: 384x640 2 ints, 2 pros, 20.5ms\n",
      "image 109/363 /home/jupyter/work/resources/test_dataset/2 (32).jpg: 384x640 1 int, 1 geo, 20.5ms\n",
      "image 110/363 /home/jupyter/work/resources/test_dataset/2 (33).jpg: 384x640 1 int, 2 geos, 20.4ms\n",
      "image 111/363 /home/jupyter/work/resources/test_dataset/2 (34).jpg: 384x640 1 geo, 20.5ms\n",
      "image 112/363 /home/jupyter/work/resources/test_dataset/2 (35).jpg: 384x640 1 geo, 20.7ms\n",
      "image 113/363 /home/jupyter/work/resources/test_dataset/2 (36).jpg: 384x640 2 geos, 20.6ms\n",
      "image 114/363 /home/jupyter/work/resources/test_dataset/2 (37).jpg: 384x640 1 int, 1 geo, 20.7ms\n",
      "image 115/363 /home/jupyter/work/resources/test_dataset/2 (38).jpg: 384x640 (no detections), 20.8ms\n",
      "image 116/363 /home/jupyter/work/resources/test_dataset/2 (39).jpg: 384x640 1 int, 20.5ms\n",
      "image 117/363 /home/jupyter/work/resources/test_dataset/2 (4).jpg: 384x640 1 geo, 20.5ms\n",
      "image 118/363 /home/jupyter/work/resources/test_dataset/2 (40).jpg: 384x640 2 geos, 20.7ms\n",
      "image 119/363 /home/jupyter/work/resources/test_dataset/2 (41).jpg: 384x640 1 int, 2 geos, 20.5ms\n",
      "image 120/363 /home/jupyter/work/resources/test_dataset/2 (42).jpg: 384x640 1 geo, 20.5ms\n",
      "image 121/363 /home/jupyter/work/resources/test_dataset/2 (43).jpg: 384x640 2 ints, 3 geos, 2 pros, 20.3ms\n",
      "image 122/363 /home/jupyter/work/resources/test_dataset/2 (44).jpg: 384x640 1 int, 20.3ms\n",
      "image 123/363 /home/jupyter/work/resources/test_dataset/2 (45).jpg: 384x640 1 int, 2 pros, 20.2ms\n",
      "image 124/363 /home/jupyter/work/resources/test_dataset/2 (46).jpg: 384x640 1 pro, 20.4ms\n",
      "image 125/363 /home/jupyter/work/resources/test_dataset/2 (47).jpg: 384x640 1 geo, 3 pros, 20.5ms\n",
      "image 126/363 /home/jupyter/work/resources/test_dataset/2 (48).jpg: 384x640 1 geo, 2 pros, 20.4ms\n",
      "image 127/363 /home/jupyter/work/resources/test_dataset/2 (49).jpg: 384x640 1 int, 1 pro, 20.3ms\n",
      "image 128/363 /home/jupyter/work/resources/test_dataset/2 (5).jpg: 384x640 1 geo, 20.4ms\n",
      "image 129/363 /home/jupyter/work/resources/test_dataset/2 (50).jpg: 384x640 3 ints, 2 geos, 4 pros, 20.4ms\n",
      "image 130/363 /home/jupyter/work/resources/test_dataset/2 (51).jpg: 384x640 2 ints, 2 pros, 20.4ms\n",
      "image 131/363 /home/jupyter/work/resources/test_dataset/2 (52).jpg: 384x640 1 int, 1 geo, 2 pros, 20.6ms\n",
      "image 132/363 /home/jupyter/work/resources/test_dataset/2 (53).jpg: 384x640 3 ints, 2 geos, 3 pros, 20.5ms\n",
      "image 133/363 /home/jupyter/work/resources/test_dataset/2 (54).jpg: 384x640 2 ints, 2 geos, 2 pros, 20.6ms\n",
      "image 134/363 /home/jupyter/work/resources/test_dataset/2 (55).jpg: 384x640 3 ints, 1 geo, 3 pros, 20.6ms\n",
      "image 135/363 /home/jupyter/work/resources/test_dataset/2 (56).jpg: 384x640 3 ints, 1 geo, 3 pros, 20.8ms\n",
      "image 136/363 /home/jupyter/work/resources/test_dataset/2 (57).jpg: 384x640 1 int, 1 geo, 1 pro, 20.6ms\n",
      "image 137/363 /home/jupyter/work/resources/test_dataset/2 (58).jpg: 384x640 2 ints, 2 pros, 21.0ms\n",
      "image 138/363 /home/jupyter/work/resources/test_dataset/2 (59).jpg: 384x640 1 int, 1 pro, 20.8ms\n",
      "image 139/363 /home/jupyter/work/resources/test_dataset/2 (6).jpg: 384x640 1 geo, 20.8ms\n",
      "image 140/363 /home/jupyter/work/resources/test_dataset/2 (60).jpg: 384x640 1 int, 1 geo, 2 pros, 20.7ms\n",
      "image 141/363 /home/jupyter/work/resources/test_dataset/2 (61).jpg: 384x640 3 geos, 20.8ms\n",
      "image 142/363 /home/jupyter/work/resources/test_dataset/2 (62).jpg: 384x640 1 geo, 3 pros, 20.6ms\n",
      "image 143/363 /home/jupyter/work/resources/test_dataset/2 (63).jpg: 384x640 4 pros, 20.5ms\n",
      "image 144/363 /home/jupyter/work/resources/test_dataset/2 (64).jpg: 384x640 1 int, 3 pros, 20.9ms\n",
      "image 145/363 /home/jupyter/work/resources/test_dataset/2 (65).jpg: 384x640 7 pros, 20.7ms\n",
      "image 146/363 /home/jupyter/work/resources/test_dataset/2 (7).jpg: 384x640 1 geo, 21.3ms\n",
      "image 147/363 /home/jupyter/work/resources/test_dataset/2 (8).jpg: 384x640 1 geo, 1 pro, 20.6ms\n",
      "image 148/363 /home/jupyter/work/resources/test_dataset/2 (9).jpg: 384x640 1 int, 1 geo, 20.5ms\n",
      "image 149/363 /home/jupyter/work/resources/test_dataset/20.jpg: 384x640 1 geo, 20.7ms\n",
      "image 150/363 /home/jupyter/work/resources/test_dataset/21.jpg: 384x640 (no detections), 20.8ms\n",
      "image 151/363 /home/jupyter/work/resources/test_dataset/22.jpg: 384x640 3 geos, 20.9ms\n",
      "image 152/363 /home/jupyter/work/resources/test_dataset/23.jpg: 384x640 1 geo, 21.0ms\n",
      "image 153/363 /home/jupyter/work/resources/test_dataset/24.jpg: 384x640 1 geo, 1 pro, 20.9ms\n",
      "image 154/363 /home/jupyter/work/resources/test_dataset/25.jpg: 384x640 (no detections), 21.1ms\n",
      "image 155/363 /home/jupyter/work/resources/test_dataset/26.jpg: 384x640 1 geo, 2 pros, 21.5ms\n",
      "image 156/363 /home/jupyter/work/resources/test_dataset/27.jpg: 384x640 (no detections), 21.4ms\n",
      "image 157/363 /home/jupyter/work/resources/test_dataset/28.jpg: 384x640 1 int, 3 pros, 21.3ms\n",
      "image 158/363 /home/jupyter/work/resources/test_dataset/29.jpg: 384x640 2 pros, 21.9ms\n",
      "image 159/363 /home/jupyter/work/resources/test_dataset/3 (1).jpg: 384x640 1 int, 2 geos, 3 pros, 21.0ms\n",
      "image 160/363 /home/jupyter/work/resources/test_dataset/3 (10).jpg: 384x640 1 adj, 2 ints, 1 geo, 20.9ms\n",
      "image 161/363 /home/jupyter/work/resources/test_dataset/3 (100).jpg: 384x640 1 int, 1 pro, 20.7ms\n",
      "image 162/363 /home/jupyter/work/resources/test_dataset/3 (101).jpg: 384x640 1 int, 1 pro, 21.0ms\n",
      "image 163/363 /home/jupyter/work/resources/test_dataset/3 (104).jpg: 384x640 1 adj, 1 int, 2 geos, 4 pros, 20.9ms\n",
      "image 164/363 /home/jupyter/work/resources/test_dataset/3 (105).jpg: 384x640 2 adjs, 1 int, 3 pros, 20.7ms\n",
      "image 165/363 /home/jupyter/work/resources/test_dataset/3 (106).jpg: 384x640 2 geos, 2 pros, 20.8ms\n",
      "image 166/363 /home/jupyter/work/resources/test_dataset/3 (107).jpg: 384x640 1 geo, 3 pros, 20.8ms\n",
      "image 167/363 /home/jupyter/work/resources/test_dataset/3 (108).jpg: 384x640 1 int, 2 pros, 20.6ms\n",
      "image 168/363 /home/jupyter/work/resources/test_dataset/3 (109).jpg: 384x640 2 adjs, 1 int, 3 pros, 20.6ms\n",
      "image 169/363 /home/jupyter/work/resources/test_dataset/3 (11).jpg: 384x640 1 adj, 1 int, 1 pro, 21.4ms\n",
      "image 170/363 /home/jupyter/work/resources/test_dataset/3 (111).jpg: 384x640 2 ints, 1 geo, 1 pro, 20.8ms\n",
      "image 171/363 /home/jupyter/work/resources/test_dataset/3 (112).jpg: 384x640 1 int, 1 geo, 20.4ms\n",
      "image 172/363 /home/jupyter/work/resources/test_dataset/3 (113).jpg: 384x640 2 ints, 1 pro, 20.6ms\n",
      "image 173/363 /home/jupyter/work/resources/test_dataset/3 (114).jpg: 384x640 2 ints, 1 pro, 20.8ms\n",
      "image 174/363 /home/jupyter/work/resources/test_dataset/3 (115).jpg: 384x640 2 ints, 20.5ms\n",
      "image 175/363 /home/jupyter/work/resources/test_dataset/3 (118).jpg: 384x640 2 ints, 20.5ms\n",
      "image 176/363 /home/jupyter/work/resources/test_dataset/3 (119).jpg: 384x640 1 int, 20.4ms\n",
      "image 177/363 /home/jupyter/work/resources/test_dataset/3 (120).jpg: 384x640 1 int, 20.6ms\n",
      "image 178/363 /home/jupyter/work/resources/test_dataset/3 (121).jpg: 384x640 1 int, 20.5ms\n",
      "image 179/363 /home/jupyter/work/resources/test_dataset/3 (124).jpg: 384x640 1 int, 1 geo, 2 pros, 20.4ms\n",
      "image 180/363 /home/jupyter/work/resources/test_dataset/3 (125).jpg: 384x640 1 int, 2 pros, 20.6ms\n",
      "image 181/363 /home/jupyter/work/resources/test_dataset/3 (126).jpg: 384x640 3 pros, 20.5ms\n",
      "image 182/363 /home/jupyter/work/resources/test_dataset/3 (127).jpg: 384x640 1 int, 2 pros, 20.7ms\n",
      "image 183/363 /home/jupyter/work/resources/test_dataset/3 (128).jpg: 384x640 1 int, 20.8ms\n",
      "image 184/363 /home/jupyter/work/resources/test_dataset/3 (129).jpg: 384x640 1 pro, 21.2ms\n",
      "image 185/363 /home/jupyter/work/resources/test_dataset/3 (130).jpg: 384x640 (no detections), 21.2ms\n",
      "image 186/363 /home/jupyter/work/resources/test_dataset/3 (131).jpg: 384x640 1 geo, 1 pro, 21.1ms\n",
      "image 187/363 /home/jupyter/work/resources/test_dataset/3 (134).jpg: 384x640 1 int, 1 geo, 2 pros, 21.4ms\n",
      "image 188/363 /home/jupyter/work/resources/test_dataset/3 (135).jpg: 384x640 3 geos, 1 pro, 21.4ms\n",
      "image 189/363 /home/jupyter/work/resources/test_dataset/3 (136).jpg: 384x640 1 int, 1 geo, 3 pros, 21.5ms\n",
      "image 190/363 /home/jupyter/work/resources/test_dataset/3 (137).jpg: 384x640 1 int, 2 geos, 3 pros, 21.6ms\n",
      "image 191/363 /home/jupyter/work/resources/test_dataset/3 (138).jpg: 384x640 2 geos, 3 pros, 21.6ms\n",
      "image 192/363 /home/jupyter/work/resources/test_dataset/3 (139).jpg: 384x640 1 adj, 1 int, 2 geos, 3 pros, 22.4ms\n",
      "image 193/363 /home/jupyter/work/resources/test_dataset/3 (14).jpg: 384x640 3 adjs, 2 ints, 1 pro, 21.1ms\n",
      "image 194/363 /home/jupyter/work/resources/test_dataset/3 (140).jpg: 384x640 1 int, 4 geos, 3 pros, 21.1ms\n",
      "image 195/363 /home/jupyter/work/resources/test_dataset/3 (141).jpg: 384x640 2 geos, 2 pros, 21.2ms\n",
      "image 196/363 /home/jupyter/work/resources/test_dataset/3 (144).jpg: 384x640 1 int, 20.8ms\n",
      "image 197/363 /home/jupyter/work/resources/test_dataset/3 (145).jpg: 384x640 1 geo, 1 pro, 21.2ms\n",
      "image 198/363 /home/jupyter/work/resources/test_dataset/3 (146).jpg: 384x640 1 adj, 1 int, 1 geo, 20.8ms\n",
      "image 199/363 /home/jupyter/work/resources/test_dataset/3 (147).jpg: 384x640 3 ints, 20.5ms\n",
      "image 200/363 /home/jupyter/work/resources/test_dataset/3 (148).jpg: 384x640 1 int, 3 geos, 20.4ms\n",
      "image 201/363 /home/jupyter/work/resources/test_dataset/3 (149).jpg: 384x640 2 geos, 20.4ms\n",
      "image 202/363 /home/jupyter/work/resources/test_dataset/3 (15).jpg: 384x640 1 adj, 2 ints, 2 pros, 20.8ms\n",
      "image 203/363 /home/jupyter/work/resources/test_dataset/3 (151).jpg: 384x640 3 ints, 4 geos, 21.2ms\n",
      "image 204/363 /home/jupyter/work/resources/test_dataset/3 (152).jpg: 384x640 1 int, 2 geos, 20.9ms\n",
      "image 205/363 /home/jupyter/work/resources/test_dataset/3 (153).jpg: 384x640 1 int, 20.6ms\n",
      "image 206/363 /home/jupyter/work/resources/test_dataset/3 (154).jpg: 384x640 1 int, 2 pros, 20.6ms\n",
      "image 207/363 /home/jupyter/work/resources/test_dataset/3 (156).jpg: 384x640 2 adjs, 3 ints, 3 pros, 20.5ms\n",
      "image 208/363 /home/jupyter/work/resources/test_dataset/3 (157).jpg: 384x640 1 int, 2 pros, 20.8ms\n",
      "image 209/363 /home/jupyter/work/resources/test_dataset/3 (158).jpg: 384x640 2 ints, 1 geo, 3 pros, 20.7ms\n",
      "image 210/363 /home/jupyter/work/resources/test_dataset/3 (159).jpg: 384x640 2 ints, 2 geos, 3 pros, 21.2ms\n",
      "image 211/363 /home/jupyter/work/resources/test_dataset/3 (16).jpg: 384x640 2 adjs, 1 int, 1 pro, 21.1ms\n",
      "image 212/363 /home/jupyter/work/resources/test_dataset/3 (162).jpg: 384x640 1 int, 2 pros, 21.0ms\n",
      "image 213/363 /home/jupyter/work/resources/test_dataset/3 (163).jpg: 384x640 3 pros, 20.9ms\n",
      "image 214/363 /home/jupyter/work/resources/test_dataset/3 (17).jpg: 384x640 1 adj, 1 int, 1 pro, 21.5ms\n",
      "image 215/363 /home/jupyter/work/resources/test_dataset/3 (18).jpg: 384x640 3 adjs, 1 int, 1 pro, 21.2ms\n",
      "image 216/363 /home/jupyter/work/resources/test_dataset/3 (19).jpg: 384x640 1 int, 1 pro, 20.9ms\n",
      "image 217/363 /home/jupyter/work/resources/test_dataset/3 (2).jpg: 384x640 3 pros, 20.8ms\n",
      "image 218/363 /home/jupyter/work/resources/test_dataset/3 (22).jpg: 384x640 2 ints, 20.5ms\n",
      "image 219/363 /home/jupyter/work/resources/test_dataset/3 (23).jpg: 384x640 1 int, 20.5ms\n",
      "image 220/363 /home/jupyter/work/resources/test_dataset/3 (24).jpg: 384x640 1 int, 21.4ms\n",
      "image 221/363 /home/jupyter/work/resources/test_dataset/3 (25).jpg: 384x640 1 int, 1 pro, 20.7ms\n",
      "image 222/363 /home/jupyter/work/resources/test_dataset/3 (26).jpg: 384x640 1 int, 1 pro, 20.6ms\n",
      "image 223/363 /home/jupyter/work/resources/test_dataset/3 (27).jpg: 384x640 1 int, 1 pro, 20.6ms\n",
      "image 224/363 /home/jupyter/work/resources/test_dataset/3 (28).jpg: 384x640 1 int, 1 pro, 20.9ms\n",
      "image 225/363 /home/jupyter/work/resources/test_dataset/3 (29).jpg: 384x640 2 ints, 3 pros, 20.5ms\n",
      "image 226/363 /home/jupyter/work/resources/test_dataset/3 (3).jpg: 384x640 1 int, 4 pros, 20.4ms\n",
      "image 227/363 /home/jupyter/work/resources/test_dataset/3 (30).jpg: 384x640 1 int, 20.5ms\n",
      "image 228/363 /home/jupyter/work/resources/test_dataset/3 (31).jpg: 384x640 1 int, 1 geo, 1 pro, 20.5ms\n",
      "image 229/363 /home/jupyter/work/resources/test_dataset/3 (32).jpg: 384x640 1 int, 1 pro, 20.6ms\n",
      "image 230/363 /home/jupyter/work/resources/test_dataset/3 (33).jpg: 384x640 1 int, 3 geos, 1 pro, 20.5ms\n",
      "image 231/363 /home/jupyter/work/resources/test_dataset/3 (34).jpg: 384x640 1 int, 1 geo, 2 pros, 20.5ms\n",
      "image 232/363 /home/jupyter/work/resources/test_dataset/3 (35).jpg: 384x640 2 geos, 20.5ms\n",
      "image 233/363 /home/jupyter/work/resources/test_dataset/3 (36).jpg: 384x640 1 int, 1 geo, 20.7ms\n",
      "image 234/363 /home/jupyter/work/resources/test_dataset/3 (37).jpg: 384x640 1 int, 1 geo, 20.8ms\n",
      "image 235/363 /home/jupyter/work/resources/test_dataset/3 (39).jpg: 384x640 1 int, 1 geo, 1 pro, 20.7ms\n",
      "image 236/363 /home/jupyter/work/resources/test_dataset/3 (4).jpg: 384x640 1 adj, 1 int, 3 pros, 20.6ms\n",
      "image 237/363 /home/jupyter/work/resources/test_dataset/3 (42).jpg: 384x640 1 int, 1 geo, 1 pro, 21.1ms\n",
      "image 238/363 /home/jupyter/work/resources/test_dataset/3 (43).jpg: 384x640 3 ints, 2 geos, 20.4ms\n",
      "image 239/363 /home/jupyter/work/resources/test_dataset/3 (44).jpg: 384x640 1 int, 20.6ms\n",
      "image 240/363 /home/jupyter/work/resources/test_dataset/3 (45).jpg: 384x640 2 ints, 4 geos, 1 pro, 20.8ms\n",
      "image 241/363 /home/jupyter/work/resources/test_dataset/3 (48).jpg: 384x640 1 adj, 2 ints, 2 pros, 20.7ms\n",
      "image 242/363 /home/jupyter/work/resources/test_dataset/3 (49).jpg: 384x640 2 ints, 1 geo, 1 pro, 20.7ms\n",
      "image 243/363 /home/jupyter/work/resources/test_dataset/3 (5).jpg: 384x640 1 int, 1 geo, 3 pros, 21.1ms\n",
      "image 244/363 /home/jupyter/work/resources/test_dataset/3 (50).jpg: 384x640 1 int, 1 geo, 1 pro, 20.5ms\n",
      "image 245/363 /home/jupyter/work/resources/test_dataset/3 (51).jpg: 384x640 1 adj, 2 pros, 20.9ms\n",
      "image 246/363 /home/jupyter/work/resources/test_dataset/3 (52).jpg: 384x640 1 int, 1 pro, 20.7ms\n",
      "image 247/363 /home/jupyter/work/resources/test_dataset/3 (53).jpg: 384x640 1 adj, 2 ints, 1 geo, 3 pros, 20.9ms\n",
      "image 248/363 /home/jupyter/work/resources/test_dataset/3 (55).jpg: 384x640 (no detections), 20.9ms\n",
      "image 249/363 /home/jupyter/work/resources/test_dataset/3 (56).jpg: 384x640 1 int, 3 pros, 20.6ms\n",
      "image 250/363 /home/jupyter/work/resources/test_dataset/3 (57).jpg: 384x640 1 int, 1 pro, 20.6ms\n",
      "image 251/363 /home/jupyter/work/resources/test_dataset/3 (58).jpg: 384x640 2 ints, 1 geo, 20.7ms\n",
      "image 252/363 /home/jupyter/work/resources/test_dataset/3 (59).jpg: 384x640 1 int, 20.6ms\n",
      "image 253/363 /home/jupyter/work/resources/test_dataset/3 (6).jpg: 384x640 2 ints, 1 geo, 1 pro, 20.7ms\n",
      "image 254/363 /home/jupyter/work/resources/test_dataset/3 (60).jpg: 384x640 1 int, 1 geo, 1 pro, 20.6ms\n",
      "image 255/363 /home/jupyter/work/resources/test_dataset/3 (61).jpg: 384x640 2 geos, 1 pro, 20.6ms\n",
      "image 256/363 /home/jupyter/work/resources/test_dataset/3 (64).jpg: 384x640 2 ints, 1 geo, 1 pro, 20.6ms\n",
      "image 257/363 /home/jupyter/work/resources/test_dataset/3 (65).jpg: 384x640 1 int, 2 geos, 1 pro, 20.5ms\n",
      "image 258/363 /home/jupyter/work/resources/test_dataset/3 (66).jpg: 384x640 3 pros, 20.6ms\n",
      "image 259/363 /home/jupyter/work/resources/test_dataset/3 (67).jpg: 384x640 1 int, 3 pros, 20.7ms\n",
      "image 260/363 /home/jupyter/work/resources/test_dataset/3 (68).jpg: 384x640 2 ints, 1 geo, 3 pros, 21.4ms\n",
      "image 261/363 /home/jupyter/work/resources/test_dataset/3 (69).jpg: 384x640 (no detections), 20.6ms\n",
      "image 262/363 /home/jupyter/work/resources/test_dataset/3 (72).jpg: 384x640 2 adjs, 4 geos, 1 pro, 20.5ms\n",
      "image 263/363 /home/jupyter/work/resources/test_dataset/3 (73).jpg: 384x640 1 int, 3 geos, 3 pros, 20.5ms\n",
      "image 264/363 /home/jupyter/work/resources/test_dataset/3 (74).jpg: 384x640 1 int, 20.6ms\n",
      "image 265/363 /home/jupyter/work/resources/test_dataset/3 (75).jpg: 384x640 1 int, 4 geos, 20.6ms\n",
      "image 266/363 /home/jupyter/work/resources/test_dataset/3 (76).jpg: 384x640 1 int, 3 geos, 20.5ms\n",
      "image 267/363 /home/jupyter/work/resources/test_dataset/3 (77).jpg: 384x640 5 geos, 1 pro, 20.6ms\n",
      "image 268/363 /home/jupyter/work/resources/test_dataset/3 (78).jpg: 384x640 1 int, 2 geos, 20.6ms\n",
      "image 269/363 /home/jupyter/work/resources/test_dataset/3 (79).jpg: 384x640 1 adj, 6 geos, 20.5ms\n",
      "image 270/363 /home/jupyter/work/resources/test_dataset/3 (8).jpg: 384x640 2 adjs, 2 ints, 1 geo, 1 pro, 20.9ms\n",
      "image 271/363 /home/jupyter/work/resources/test_dataset/3 (81).jpg: 384x640 1 int, 1 pro, 22.3ms\n",
      "image 272/363 /home/jupyter/work/resources/test_dataset/3 (82).jpg: 384x640 1 int, 2 geos, 1 pro, 21.8ms\n",
      "image 273/363 /home/jupyter/work/resources/test_dataset/3 (83).jpg: 384x640 1 int, 1 pro, 21.8ms\n",
      "image 274/363 /home/jupyter/work/resources/test_dataset/3 (84).jpg: 384x640 2 adjs, 3 geos, 2 pros, 21.5ms\n",
      "image 275/363 /home/jupyter/work/resources/test_dataset/3 (85).jpg: 384x640 1 adj, 1 int, 2 pros, 21.8ms\n",
      "image 276/363 /home/jupyter/work/resources/test_dataset/3 (88).jpg: 384x640 1 pro, 21.5ms\n",
      "image 277/363 /home/jupyter/work/resources/test_dataset/3 (89).jpg: 384x640 1 adj, 1 int, 1 geo, 1 pro, 21.4ms\n",
      "image 278/363 /home/jupyter/work/resources/test_dataset/3 (9).jpg: 384x640 1 adj, 3 ints, 1 geo, 2 pros, 21.5ms\n",
      "image 279/363 /home/jupyter/work/resources/test_dataset/3 (90).jpg: 384x640 1 adj, 1 int, 1 geo, 1 pro, 21.5ms\n",
      "image 280/363 /home/jupyter/work/resources/test_dataset/3 (91).jpg: 384x640 2 adjs, 2 ints, 2 pros, 21.2ms\n",
      "image 281/363 /home/jupyter/work/resources/test_dataset/3 (92).jpg: 384x640 2 ints, 1 pro, 21.0ms\n",
      "image 282/363 /home/jupyter/work/resources/test_dataset/3 (93).jpg: 384x640 1 adj, 1 int, 1 pro, 21.9ms\n",
      "image 283/363 /home/jupyter/work/resources/test_dataset/3 (96).jpg: 384x640 1 int, 1 geo, 3 pros, 21.3ms\n",
      "image 284/363 /home/jupyter/work/resources/test_dataset/3 (97).jpg: 384x640 1 int, 1 pro, 21.2ms\n",
      "image 285/363 /home/jupyter/work/resources/test_dataset/3 (98).jpg: 384x640 2 adjs, 2 ints, 3 pros, 20.7ms\n",
      "image 286/363 /home/jupyter/work/resources/test_dataset/3 (99).jpg: 384x640 2 adjs, 1 int, 4 pros, 21.0ms\n",
      "image 287/363 /home/jupyter/work/resources/test_dataset/3.jpg: 384x640 (no detections), 21.0ms\n",
      "image 288/363 /home/jupyter/work/resources/test_dataset/30.jpg: 384x640 1 int, 3 geos, 2 pros, 21.1ms\n",
      "image 289/363 /home/jupyter/work/resources/test_dataset/31.jpg: 384x640 2 geos, 21.5ms\n",
      "image 290/363 /home/jupyter/work/resources/test_dataset/32.jpg: 384x640 1 geo, 1 pro, 21.3ms\n",
      "image 291/363 /home/jupyter/work/resources/test_dataset/33.jpg: 384x640 (no detections), 21.2ms\n",
      "image 292/363 /home/jupyter/work/resources/test_dataset/34.jpg: 384x640 (no detections), 21.2ms\n",
      "image 293/363 /home/jupyter/work/resources/test_dataset/35.jpg: 384x640 (no detections), 21.2ms\n",
      "image 294/363 /home/jupyter/work/resources/test_dataset/36.jpg: 384x640 1 pro, 21.0ms\n",
      "image 295/363 /home/jupyter/work/resources/test_dataset/37.jpg: 384x640 (no detections), 21.0ms\n",
      "image 296/363 /home/jupyter/work/resources/test_dataset/38.jpg: 384x640 1 geo, 21.1ms\n",
      "image 297/363 /home/jupyter/work/resources/test_dataset/39.jpg: 384x640 1 int, 1 geo, 21.0ms\n",
      "image 298/363 /home/jupyter/work/resources/test_dataset/4.jpg: 384x640 1 geo, 22.3ms\n",
      "image 299/363 /home/jupyter/work/resources/test_dataset/40.jpg: 384x640 1 int, 21.2ms\n",
      "image 300/363 /home/jupyter/work/resources/test_dataset/41.jpg: 384x640 2 geos, 1 pro, 21.3ms\n",
      "image 301/363 /home/jupyter/work/resources/test_dataset/42.jpg: 384x640 (no detections), 21.1ms\n",
      "image 302/363 /home/jupyter/work/resources/test_dataset/43.jpg: 384x640 1 geo, 21.3ms\n",
      "image 303/363 /home/jupyter/work/resources/test_dataset/44.jpg: 384x640 (no detections), 21.2ms\n",
      "image 304/363 /home/jupyter/work/resources/test_dataset/45.jpg: 384x640 1 pro, 21.2ms\n",
      "image 305/363 /home/jupyter/work/resources/test_dataset/46.jpg: 384x640 1 adj, 1 int, 3 pros, 21.2ms\n",
      "image 306/363 /home/jupyter/work/resources/test_dataset/47.jpg: 384x640 (no detections), 21.4ms\n",
      "image 307/363 /home/jupyter/work/resources/test_dataset/48.jpg: 384x640 2 ints, 1 pro, 21.2ms\n",
      "image 308/363 /home/jupyter/work/resources/test_dataset/49.jpg: 384x640 1 geo, 21.1ms\n",
      "image 309/363 /home/jupyter/work/resources/test_dataset/5.jpg: 384x640 1 int, 21.2ms\n",
      "image 310/363 /home/jupyter/work/resources/test_dataset/50.jpg: 384x640 (no detections), 21.3ms\n",
      "image 311/363 /home/jupyter/work/resources/test_dataset/51.jpg: 384x640 3 pros, 21.5ms\n",
      "image 312/363 /home/jupyter/work/resources/test_dataset/52.jpg: 384x640 1 geo, 21.4ms\n",
      "image 313/363 /home/jupyter/work/resources/test_dataset/53.jpg: 384x640 1 int, 1 geo, 22.4ms\n",
      "image 314/363 /home/jupyter/work/resources/test_dataset/54.jpg: 384x640 1 geo, 21.5ms\n",
      "image 315/363 /home/jupyter/work/resources/test_dataset/55.jpg: 384x640 1 adj, 1 geo, 1 pro, 21.5ms\n",
      "image 316/363 /home/jupyter/work/resources/test_dataset/56.jpg: 384x640 1 pro, 21.6ms\n",
      "image 317/363 /home/jupyter/work/resources/test_dataset/57.jpg: 384x640 1 adj, 1 pro, 21.3ms\n",
      "image 318/363 /home/jupyter/work/resources/test_dataset/58.jpg: 384x640 1 int, 1 geo, 1 pro, 21.0ms\n",
      "image 319/363 /home/jupyter/work/resources/test_dataset/59.jpg: 384x640 1 geo, 1 pro, 21.6ms\n",
      "image 320/363 /home/jupyter/work/resources/test_dataset/6.jpg: 384x640 3 ints, 21.4ms\n",
      "image 321/363 /home/jupyter/work/resources/test_dataset/60.jpg: 384x640 1 geo, 1 pro, 21.7ms\n",
      "image 322/363 /home/jupyter/work/resources/test_dataset/61.jpg: 384x640 2 pros, 21.7ms\n",
      "image 323/363 /home/jupyter/work/resources/test_dataset/62.jpg: 384x640 1 int, 21.6ms\n",
      "image 324/363 /home/jupyter/work/resources/test_dataset/63.jpg: 384x640 1 geo, 21.3ms\n",
      "image 325/363 /home/jupyter/work/resources/test_dataset/64.jpg: 384x640 (no detections), 21.3ms\n",
      "image 326/363 /home/jupyter/work/resources/test_dataset/65.jpg: 384x640 1 int, 1 geo, 1 pro, 21.3ms\n",
      "image 327/363 /home/jupyter/work/resources/test_dataset/66.jpg: 384x640 1 adj, 2 geos, 1 pro, 21.4ms\n",
      "image 328/363 /home/jupyter/work/resources/test_dataset/67.jpg: 384x640 (no detections), 21.6ms\n",
      "image 329/363 /home/jupyter/work/resources/test_dataset/68.jpg: 384x640 (no detections), 21.2ms\n",
      "image 330/363 /home/jupyter/work/resources/test_dataset/69.jpg: 384x640 (no detections), 21.5ms\n",
      "image 331/363 /home/jupyter/work/resources/test_dataset/7.jpg: 384x640 1 int, 21.0ms\n",
      "image 332/363 /home/jupyter/work/resources/test_dataset/70.jpg: 384x640 1 geo, 21.4ms\n",
      "image 333/363 /home/jupyter/work/resources/test_dataset/71.jpg: 384x640 (no detections), 21.2ms\n",
      "image 334/363 /home/jupyter/work/resources/test_dataset/72.jpg: 384x640 2 ints, 1 geo, 21.3ms\n",
      "image 335/363 /home/jupyter/work/resources/test_dataset/73.jpg: 384x640 1 int, 22.4ms\n",
      "image 336/363 /home/jupyter/work/resources/test_dataset/74.jpg: 384x640 1 int, 22.3ms\n",
      "image 337/363 /home/jupyter/work/resources/test_dataset/75.jpg: 384x640 2 ints, 22.2ms\n",
      "image 338/363 /home/jupyter/work/resources/test_dataset/76.jpg: 384x640 3 ints, 2 pros, 22.6ms\n",
      "image 339/363 /home/jupyter/work/resources/test_dataset/77.jpg: 384x640 (no detections), 22.4ms\n",
      "image 340/363 /home/jupyter/work/resources/test_dataset/78.jpg: 384x640 1 int, 1 geo, 22.0ms\n",
      "image 341/363 /home/jupyter/work/resources/test_dataset/79.jpg: 384x640 1 adj, 21.9ms\n",
      "image 342/363 /home/jupyter/work/resources/test_dataset/8.jpg: 384x640 1 int, 1 geo, 22.0ms\n",
      "image 343/363 /home/jupyter/work/resources/test_dataset/80.jpg: 384x640 (no detections), 21.9ms\n",
      "image 344/363 /home/jupyter/work/resources/test_dataset/81.jpg: 384x640 2 ints, 2 pros, 21.9ms\n",
      "image 345/363 /home/jupyter/work/resources/test_dataset/82.jpg: 384x640 1 pro, 21.8ms\n",
      "image 346/363 /home/jupyter/work/resources/test_dataset/83.jpg: 384x640 1 pro, 22.8ms\n",
      "image 347/363 /home/jupyter/work/resources/test_dataset/84.jpg: 384x640 (no detections), 21.8ms\n",
      "image 348/363 /home/jupyter/work/resources/test_dataset/85.jpg: 384x640 1 int, 1 pro, 21.8ms\n",
      "image 349/363 /home/jupyter/work/resources/test_dataset/86.jpg: 384x640 1 int, 3 pros, 21.9ms\n",
      "image 350/363 /home/jupyter/work/resources/test_dataset/87.jpg: 384x640 1 geo, 2 pros, 22.0ms\n",
      "image 351/363 /home/jupyter/work/resources/test_dataset/88.jpg: 384x640 1 int, 1 pro, 22.0ms\n",
      "image 352/363 /home/jupyter/work/resources/test_dataset/89.jpg: 384x640 (no detections), 22.0ms\n",
      "image 353/363 /home/jupyter/work/resources/test_dataset/9.jpg: 384x640 3 ints, 22.0ms\n",
      "image 354/363 /home/jupyter/work/resources/test_dataset/90.jpg: 384x640 1 adj, 1 int, 2 pros, 22.0ms\n",
      "image 355/363 /home/jupyter/work/resources/test_dataset/91.jpg: 384x640 (no detections), 22.3ms\n",
      "image 356/363 /home/jupyter/work/resources/test_dataset/92.jpg: 384x640 1 int, 22.5ms\n",
      "image 357/363 /home/jupyter/work/resources/test_dataset/93.jpg: 384x640 1 adj, 2 ints, 22.3ms\n",
      "image 358/363 /home/jupyter/work/resources/test_dataset/94.jpg: 384x640 1 adj, 1 int, 22.3ms\n",
      "image 359/363 /home/jupyter/work/resources/test_dataset/95.jpg: 384x640 1 int, 22.1ms\n",
      "image 360/363 /home/jupyter/work/resources/test_dataset/96.jpg: 384x640 (no detections), 21.9ms\n",
      "image 361/363 /home/jupyter/work/resources/test_dataset/97.jpg: 384x640 (no detections), 21.8ms\n",
      "image 362/363 /home/jupyter/work/resources/test_dataset/98.jpg: 384x640 (no detections), 21.7ms\n",
      "image 363/363 /home/jupyter/work/resources/test_dataset/99.jpg: 384x640 2 ints, 21.6ms\n",
      "Speed: 0.4ms pre-process, 21.1ms inference, 0.9ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1myolov9/runs/detect/exp38\u001b[0m\n",
      "319 labels saved to yolov9/runs/detect/exp38/labels\n"
     ]
    }
   ],
   "source": [
    "from yolov9.detect_dual import run\n",
    "\n",
    "run(\n",
    "    weights='/home/jupyter/work/resources/yolov9/runs/train/exp4/weights/best.pt',\n",
    "    conf_thres=0.1,\n",
    "    imgsz=(640,640),\n",
    "    source='/home/jupyter/work/resources/test_dataset',\n",
    "    save_txt=True,\n",
    "    nosave=True,\n",
    "    device='0'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T05:47:45.306326Z",
     "iopub.status.busy": "2024-06-16T05:47:45.305226Z",
     "iopub.status.idle": "2024-06-16T05:47:45.324957Z",
     "shell.execute_reply": "2024-06-16T05:47:45.324123Z",
     "shell.execute_reply.started": "2024-06-16T05:47:45.306292Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/work/resources/yolov9\n"
     ]
    }
   ],
   "source": [
    "%cd yolov9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T05:47:56.001861Z",
     "iopub.status.busy": "2024-06-16T05:47:56.000548Z",
     "iopub.status.idle": "2024-06-16T05:47:56.177094Z",
     "shell.execute_reply": "2024-06-16T05:47:56.176275Z",
     "shell.execute_reply.started": "2024-06-16T05:47:56.001818Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data = []\n",
    "folder_path = 'runs/detect/exp38/labels'\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".txt\"):\n",
    "        image_name = file_name.replace(\".txt\", \".jpg\")\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "        with open(file_path, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "                \n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                class_id, rel_x, rel_y, width, height = parts\n",
    "                data.append((image_name, int(class_id), float(rel_x), float(rel_y), float(width), float(height)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T05:47:59.270328Z",
     "iopub.status.busy": "2024-06-16T05:47:59.269186Z",
     "iopub.status.idle": "2024-06-16T05:47:59.292511Z",
     "shell.execute_reply": "2024-06-16T05:47:59.291680Z",
     "shell.execute_reply.started": "2024-06-16T05:47:59.270294Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data, columns=['filename','class_id','rel_x','rel_y','width','height'])\n",
    "\n",
    "df.to_csv(\"sumbission20.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T02:38:53.290451Z",
     "iopub.status.busy": "2024-06-16T02:38:53.289337Z",
     "iopub.status.idle": "2024-06-16T02:39:34.823790Z",
     "shell.execute_reply": "2024-06-16T02:39:34.822910Z",
     "shell.execute_reply.started": "2024-06-16T02:38:53.290411Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval_dual: \u001b[0mdata=velding_defects/data.yaml, weights=['runs/train/exp14/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.7, max_det=300, task=val, device=0, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False, min_items=0\n",
      "YOLO 🚀 2024-6-15 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla V100-SXM2-32GB, 32501MiB)\n",
      "\n",
      "Fusing layers... \n",
      "yolov9-e summary: 839 layers, 68553982 parameters, 0 gradients, 240.7 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/jupyter/work/resources/yolov9/velding_defects/val.cache... 1046 images, 137 backgrounds, 0 corrupt: 100%|██████████| 1046/1046 00:00\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 33/33 00:28\n",
      "                   all       1046       3051      0.947       0.93      0.964      0.609\n",
      "                   adj       1046       1338      0.921      0.925      0.937      0.493\n",
      "                   int       1046        368       0.95      0.826      0.921       0.55\n",
      "                   geo       1046        937      0.961      0.975      0.991      0.736\n",
      "                   pro       1046        248      0.942      0.979      0.984      0.581\n",
      "                   non       1046        160      0.962      0.944       0.99      0.685\n",
      "Speed: 0.1ms pre-process, 16.5ms inference, 0.9ms NMS per image at shape (32, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/val/exp\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 val_dual.py \\\n",
    "--img 640 --batch 32 --conf 0.001 --iou 0.7 --device 0 \\\n",
    "--data velding_defects/data.yaml \\\n",
    "--weights runs/train/exp14/weights/best.pt\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
